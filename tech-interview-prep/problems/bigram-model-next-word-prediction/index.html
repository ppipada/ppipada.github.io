<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Bigram Model - Next Word Prediction# Levels: level-4 Data structures: hash-table, string, array Patterns: hashing Practice Link# This is a standard “bigram / Markov chain” exercise (not a single canonical LeetCode). Description# Input: a list of sentences, where each sentence is a list of words/tokens. Build a model that counts which words follow a given word (a bigram frequency table). Query: given a word w, predict a next word: Option A (deterministic): return the most frequent next word Option B (probabilistic): sample a next word proportional to its observed frequency If w was never seen (or has no following word), return an empty string. Example# 1Training data: 2[ 3 [&#34;I&#34;, &#34;am&#34;, &#34;sam&#34;], 4 [&#34;sam&#34;, &#34;i&#34;, &#34;am&#34;], 5 [&#34;i&#34;, &#34;like&#34;, &#34;green&#34;, &#34;eggs&#34;, &#34;and&#34;, &#34;ham&#34;] 6] 7 8Possible: 9most_common_next(&#34;i&#34;) -&gt; &#34;am&#34;Python Solution# 1from collections import Counter 2from collections import defaultdict 3import random 4from typing import DefaultDict, Dict, List, Optional 5 6BigramModel = DefaultDict[str, Counter] 7 8def train_bigrams(sentences: List[List[str]]) -&gt; BigramModel: 9 &#34;&#34;&#34; 10 Build next-word counts: 11 model[w][next_w] &#43;= 1 12 13 Time: O(total tokens) 14 Space: O(number of observed bigrams) 15 &#34;&#34;&#34; 16 model: BigramModel = defaultdict(Counter) 17 18 for sent in sentences or []: 19 if not sent: 20 continue 21 for i in range(len(sent) - 1): 22 w, nxt = sent[i], sent[i &#43; 1] 23 model[w][nxt] &#43;= 1 24 25 return model 26 27def most_common_next(w: str, model: BigramModel) -&gt; str: 28 &#34;&#34;&#34; 29 Return the most frequent next token after w, else &#34;&#34;. 30 &#34;&#34;&#34; 31 if not w or w not in model or not model[w]: 32 return &#34;&#34; 33 # Counter.most_common(1) returns [(word, count)] 34 return model[w].most_common(1)[0][0] 35 36def sample_next(w: str, model: BigramModel, rng: Optional[random.Random] = None) -&gt; str: 37 &#34;&#34;&#34; 38 Sample next token proportional to observed frequencies, else &#34;&#34;. 39 &#34;&#34;&#34; 40 if not w or w not in model or not model[w]: 41 return &#34;&#34; 42 rng = rng or random.Random() 43 44 counter = model[w] 45 total = sum(counter.values()) 46 r = rng.randrange(total) # integer in [0, total-1] 47 48 cum = 0 49 for nxt, c in counter.items(): 50 cum &#43;= c 51 if r &lt; cum: 52 return nxt 53 54 return &#34;&#34; # defensive; shouldn&#39;t happen">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="https://pankajpipada.com/tech-interview-prep/problems/bigram-model-next-word-prediction/">
  <meta property="og:site_name" content="Tech Interview Prep">
  <meta property="og:title" content="Bigram Model - Next Word Prediction">
  <meta property="og:description" content="Bigram Model - Next Word Prediction# Levels: level-4 Data structures: hash-table, string, array Patterns: hashing Practice Link# This is a standard “bigram / Markov chain” exercise (not a single canonical LeetCode). Description# Input: a list of sentences, where each sentence is a list of words/tokens. Build a model that counts which words follow a given word (a bigram frequency table). Query: given a word w, predict a next word: Option A (deterministic): return the most frequent next word Option B (probabilistic): sample a next word proportional to its observed frequency If w was never seen (or has no following word), return an empty string. Example# 1Training data: 2[ 3 [&#34;I&#34;, &#34;am&#34;, &#34;sam&#34;], 4 [&#34;sam&#34;, &#34;i&#34;, &#34;am&#34;], 5 [&#34;i&#34;, &#34;like&#34;, &#34;green&#34;, &#34;eggs&#34;, &#34;and&#34;, &#34;ham&#34;] 6] 7 8Possible: 9most_common_next(&#34;i&#34;) -&gt; &#34;am&#34;Python Solution# 1from collections import Counter 2from collections import defaultdict 3import random 4from typing import DefaultDict, Dict, List, Optional 5 6BigramModel = DefaultDict[str, Counter] 7 8def train_bigrams(sentences: List[List[str]]) -&gt; BigramModel: 9 &#34;&#34;&#34; 10 Build next-word counts: 11 model[w][next_w] &#43;= 1 12 13 Time: O(total tokens) 14 Space: O(number of observed bigrams) 15 &#34;&#34;&#34; 16 model: BigramModel = defaultdict(Counter) 17 18 for sent in sentences or []: 19 if not sent: 20 continue 21 for i in range(len(sent) - 1): 22 w, nxt = sent[i], sent[i &#43; 1] 23 model[w][nxt] &#43;= 1 24 25 return model 26 27def most_common_next(w: str, model: BigramModel) -&gt; str: 28 &#34;&#34;&#34; 29 Return the most frequent next token after w, else &#34;&#34;. 30 &#34;&#34;&#34; 31 if not w or w not in model or not model[w]: 32 return &#34;&#34; 33 # Counter.most_common(1) returns [(word, count)] 34 return model[w].most_common(1)[0][0] 35 36def sample_next(w: str, model: BigramModel, rng: Optional[random.Random] = None) -&gt; str: 37 &#34;&#34;&#34; 38 Sample next token proportional to observed frequencies, else &#34;&#34;. 39 &#34;&#34;&#34; 40 if not w or w not in model or not model[w]: 41 return &#34;&#34; 42 rng = rng or random.Random() 43 44 counter = model[w] 45 total = sum(counter.values()) 46 r = rng.randrange(total) # integer in [0, total-1] 47 48 cum = 0 49 for nxt, c in counter.items(): 50 cum &#43;= c 51 if r &lt; cum: 52 return nxt 53 54 return &#34;&#34; # defensive; shouldn&#39;t happen">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="website">


  <meta itemprop="name" content="Bigram Model - Next Word Prediction">
  <meta itemprop="description" content="Bigram Model - Next Word Prediction# Levels: level-4 Data structures: hash-table, string, array Patterns: hashing Practice Link# This is a standard “bigram / Markov chain” exercise (not a single canonical LeetCode). Description# Input: a list of sentences, where each sentence is a list of words/tokens. Build a model that counts which words follow a given word (a bigram frequency table). Query: given a word w, predict a next word: Option A (deterministic): return the most frequent next word Option B (probabilistic): sample a next word proportional to its observed frequency If w was never seen (or has no following word), return an empty string. Example# 1Training data: 2[ 3 [&#34;I&#34;, &#34;am&#34;, &#34;sam&#34;], 4 [&#34;sam&#34;, &#34;i&#34;, &#34;am&#34;], 5 [&#34;i&#34;, &#34;like&#34;, &#34;green&#34;, &#34;eggs&#34;, &#34;and&#34;, &#34;ham&#34;] 6] 7 8Possible: 9most_common_next(&#34;i&#34;) -&gt; &#34;am&#34;Python Solution# 1from collections import Counter 2from collections import defaultdict 3import random 4from typing import DefaultDict, Dict, List, Optional 5 6BigramModel = DefaultDict[str, Counter] 7 8def train_bigrams(sentences: List[List[str]]) -&gt; BigramModel: 9 &#34;&#34;&#34; 10 Build next-word counts: 11 model[w][next_w] &#43;= 1 12 13 Time: O(total tokens) 14 Space: O(number of observed bigrams) 15 &#34;&#34;&#34; 16 model: BigramModel = defaultdict(Counter) 17 18 for sent in sentences or []: 19 if not sent: 20 continue 21 for i in range(len(sent) - 1): 22 w, nxt = sent[i], sent[i &#43; 1] 23 model[w][nxt] &#43;= 1 24 25 return model 26 27def most_common_next(w: str, model: BigramModel) -&gt; str: 28 &#34;&#34;&#34; 29 Return the most frequent next token after w, else &#34;&#34;. 30 &#34;&#34;&#34; 31 if not w or w not in model or not model[w]: 32 return &#34;&#34; 33 # Counter.most_common(1) returns [(word, count)] 34 return model[w].most_common(1)[0][0] 35 36def sample_next(w: str, model: BigramModel, rng: Optional[random.Random] = None) -&gt; str: 37 &#34;&#34;&#34; 38 Sample next token proportional to observed frequencies, else &#34;&#34;. 39 &#34;&#34;&#34; 40 if not w or w not in model or not model[w]: 41 return &#34;&#34; 42 rng = rng or random.Random() 43 44 counter = model[w] 45 total = sum(counter.values()) 46 r = rng.randrange(total) # integer in [0, total-1] 47 48 cum = 0 49 for nxt, c in counter.items(): 50 cum &#43;= c 51 if r &lt; cum: 52 return nxt 53 54 return &#34;&#34; # defensive; shouldn&#39;t happen">
  <meta itemprop="wordCount" content="361">
  <meta itemprop="keywords" content="Hash-Table,String,Array,Level-4,Hashing">

<title>Bigram Model - Next Word Prediction | Tech Interview Prep</title>
<link rel="icon" href="/tech-interview-prep/favicon.ico" >
<link rel="manifest" href="/tech-interview-prep/manifest.json">
<link rel="canonical" href="https://pankajpipada.com/tech-interview-prep/problems/bigram-model-next-word-prediction/">
<link rel="stylesheet" href="/tech-interview-prep/book.min.a567a91bee849fdc8e389499c30737d9ada1b560975fb536f0b49c53aa5a0619.css" integrity="sha256-pWepG&#43;6En9yOOJSZwwc32a2htWCXX7U28LScU6paBhk=" crossorigin="anonymous">


  <script defer src="/tech-interview-prep/fuse.min.js"></script>
  <script defer src="/tech-interview-prep/en.search.min.713e407e217a317ce31d77e165ee40078abbdb00815736255b852dabe752b6c5.js" integrity="sha256-cT5AfiF6MXzjHXfhZe5AB4q72wCBVzYlW4Utq&#43;dStsU=" crossorigin="anonymous"></script>

		<script async src="https://www.googletagmanager.com/gtag/js?id=G-PVSXVYRTKF"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-PVSXVYRTKF');
        </script>

<link rel="alternate" type="application/rss+xml" href="https://pankajpipada.com/tech-interview-prep/problems/bigram-model-next-word-prediction/index.xml" title="Tech Interview Prep" />

  
</head>
<body dir="ltr" class="book-kind-section book-type-problems">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    
<aside class="book-menu">
  <div class="book-menu-content">
    
  <nav>
<h4 class="scramblings-brand"><a
		class="flex align-center"
		href="
			https://pankajpipada.com/
		"
	><img src="/tech-interview-prep/icon-32x32.png" alt="Logo" /><span class="text-sm">Scramblings</span>
	</a>
</h4>


<div class="book-search hidden">
  <input id="book-search-input" type="text" 
    placeholder="Search"
    aria-label="Search"
    maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>




<ul>
	<li>
		<a href="/tech-interview-prep/">
			<span>Tech Interview Prep</span>
		</a>
	</li>
</ul>







  



  
	
	<ul>
		
			
				<li>
					
	
	
	

	
		<a href="/tech-interview-prep/levels/problems/" class="">
			Path: Levels</a>
	

					
	
	<ul>
		
			
				<li>
					
	
	
	

	
		<a href="/tech-interview-prep/levels/problems/" class="">
			Problems</a>
	

				</li>
			
		
	</ul>

				</li>
			
		
			
				<li>
					
	
	
	

	
		<a href="/tech-interview-prep/patterns/problems/" class="">
			Path: Patterns</a>
	

					
	
	<ul>
		
			
				<li>
					
	
	
	

	
		<a href="/tech-interview-prep/patterns/problems/" class="">
			Problems</a>
	

				</li>
			
		
	</ul>

				</li>
			
		
			
				<li>
					
	
	
	

	
		<a href="/tech-interview-prep/data-structures/quick-notes/" class="">
			Path: Data Structures</a>
	

					
	
	<ul>
		
			
				<li>
					
	
	
	

	
		<a href="/tech-interview-prep/data-structures/quick-notes/" class="">
			Quick Notes</a>
	

				</li>
			
		
			
				<li>
					
	
	
	

	
		<a href="/tech-interview-prep/data-structures/problems/" class="">
			Problems</a>
	

				</li>
			
		
	</ul>

				</li>
			
		
			
				<li>
					
	
	
	

	
		<a href="/tech-interview-prep/algorithms/quick-notes/" class="">
			Path: Algorithms</a>
	

					
	
	<ul>
		
			
				<li>
					
	
	
	

	
		<a href="/tech-interview-prep/algorithms/quick-notes/" class="">
			Quick Notes</a>
	

				</li>
			
		
			
				<li>
					
	
	
	

	
		<a href="/tech-interview-prep/algorithms/problems/" class="">
			Problems</a>
	

				</li>
			
		
	</ul>

				</li>
			
		
			
				<li>
					
	
	
	

	
		<a href="/tech-interview-prep/problems/" class="">
			All Problems</a>
	

					
	
	<ul>
		
	</ul>

				</li>
			
		
			
				<li>
					
	
	
	

	
		<a href="/tech-interview-prep/system-design/" class="">
			System Design</a>
	

				</li>
			
		
			
				<li>
					
	
	
	

	
		<a href="/tech-interview-prep/external/" class="">
			External References</a>
	

				</li>
			
		
	</ul>













</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>



  </div>
</aside>
 

    <div class="book-page">
      <header class="book-header hidden">
        
  <div class="flex align-center justify-between">
	<label for="menu-control">
		<img src="/tech-interview-prep/icons/menu.svg" class="book-icon" alt="Menu" />
	</label>

	<label for="toc-control">
		
	</label>
</div>


  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="bigram-model---next-word-prediction">Bigram Model - Next Word Prediction<a class="anchor" href="#bigram-model---next-word-prediction">#</a></h1>
<div style="margin: .5rem 0 1rem 0;"><div>
				<strong>Levels:&ensp;</strong><a href="https://pankajpipada.com/tech-interview-prep/levels/problems/#level-4">level-4</a></div><div>
				<strong>Data structures:&ensp;</strong><a href="https://pankajpipada.com/tech-interview-prep/data-structures/problems/#hash-table">hash-table</a>,&ensp;<a href="https://pankajpipada.com/tech-interview-prep/data-structures/problems/#string">string</a>,&ensp;<a href="https://pankajpipada.com/tech-interview-prep/data-structures/problems/#array">array</a></div><div>
				<strong>Patterns:&ensp;</strong><a href="https://pankajpipada.com/tech-interview-prep/patterns/problems/#hashing">hashing</a></div></div>

<h2 id="practice-link">Practice Link<a class="anchor" href="#practice-link">#</a></h2>
<ul>
<li>This is a standard “bigram / Markov chain” exercise (not a single canonical LeetCode).</li>
</ul>
<h2 id="description">Description<a class="anchor" href="#description">#</a></h2>
<ul>
<li>Input: a list of sentences, where each sentence is a list of words/tokens.</li>
<li>Build a model that counts which words follow a given word (a bigram frequency table).</li>
<li>Query: given a word <code>w</code>, predict a next word:
<ul>
<li>Option A (deterministic): return the most frequent next word</li>
<li>Option B (probabilistic): sample a next word proportional to its observed frequency</li>
</ul>
</li>
<li>If <code>w</code> was never seen (or has no following word), return an empty string.</li>
</ul>
<h2 id="example">Example<a class="anchor" href="#example">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-text" data-lang="text"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>Training data:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2</span><span>[
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3</span><span>  [&#34;I&#34;, &#34;am&#34;, &#34;sam&#34;],
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4</span><span>  [&#34;sam&#34;, &#34;i&#34;, &#34;am&#34;],
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5</span><span>  [&#34;i&#34;, &#34;like&#34;, &#34;green&#34;, &#34;eggs&#34;, &#34;and&#34;, &#34;ham&#34;]
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6</span><span>]
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8</span><span>Possible:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">9</span><span>most_common_next(&#34;i&#34;) -&gt; &#34;am&#34;</span></span></code></pre></div><h2 id="python-solution">Python Solution<a class="anchor" href="#python-solution">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-py" data-lang="py"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1</span><span><span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> Counter
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2</span><span><span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> defaultdict
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3</span><span><span style="color:#f92672">import</span> random
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4</span><span><span style="color:#f92672">from</span> typing <span style="color:#f92672">import</span> DefaultDict, Dict, List, Optional
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6</span><span>BigramModel <span style="color:#f92672">=</span> DefaultDict[str, Counter]
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8</span><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_bigrams</span>(sentences: List[List[str]]) <span style="color:#f92672">-&gt;</span> BigramModel:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9</span><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10</span><span><span style="color:#e6db74">    Build next-word counts:
</span></span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11</span><span><span style="color:#e6db74">      model[w][next_w] += 1
</span></span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12</span><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13</span><span><span style="color:#e6db74">    Time:  O(total tokens)
</span></span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14</span><span><span style="color:#e6db74">    Space: O(number of observed bigrams)
</span></span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15</span><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16</span><span>    model: BigramModel <span style="color:#f92672">=</span> defaultdict(Counter)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18</span><span>    <span style="color:#66d9ef">for</span> sent <span style="color:#f92672">in</span> sentences <span style="color:#f92672">or</span> []:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19</span><span>        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> sent:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20</span><span>            <span style="color:#66d9ef">continue</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21</span><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(sent) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22</span><span>            w, nxt <span style="color:#f92672">=</span> sent[i], sent[i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23</span><span>            model[w][nxt] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25</span><span>    <span style="color:#66d9ef">return</span> model
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27</span><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">most_common_next</span>(w: str, model: BigramModel) <span style="color:#f92672">-&gt;</span> str:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28</span><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29</span><span><span style="color:#e6db74">    Return the most frequent next token after w, else &#34;&#34;.
</span></span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30</span><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31</span><span>    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> w <span style="color:#f92672">or</span> w <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> model <span style="color:#f92672">or</span> <span style="color:#f92672">not</span> model[w]:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32</span><span>        <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33</span><span>    <span style="color:#75715e"># Counter.most_common(1) returns [(word, count)]</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34</span><span>    <span style="color:#66d9ef">return</span> model[w]<span style="color:#f92672">.</span>most_common(<span style="color:#ae81ff">1</span>)[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36</span><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sample_next</span>(w: str, model: BigramModel, rng: Optional[random<span style="color:#f92672">.</span>Random] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>) <span style="color:#f92672">-&gt;</span> str:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37</span><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38</span><span><span style="color:#e6db74">    Sample next token proportional to observed frequencies, else &#34;&#34;.
</span></span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39</span><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40</span><span>    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> w <span style="color:#f92672">or</span> w <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> model <span style="color:#f92672">or</span> <span style="color:#f92672">not</span> model[w]:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41</span><span>        <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42</span><span>    rng <span style="color:#f92672">=</span> rng <span style="color:#f92672">or</span> random<span style="color:#f92672">.</span>Random()
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44</span><span>    counter <span style="color:#f92672">=</span> model[w]
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45</span><span>    total <span style="color:#f92672">=</span> sum(counter<span style="color:#f92672">.</span>values())
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46</span><span>    r <span style="color:#f92672">=</span> rng<span style="color:#f92672">.</span>randrange(total)  <span style="color:#75715e"># integer in [0, total-1]</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48</span><span>    cum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49</span><span>    <span style="color:#66d9ef">for</span> nxt, c <span style="color:#f92672">in</span> counter<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50</span><span>        cum <span style="color:#f92672">+=</span> c
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51</span><span>        <span style="color:#66d9ef">if</span> r <span style="color:#f92672">&lt;</span> cum:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52</span><span>            <span style="color:#66d9ef">return</span> nxt
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54</span><span>    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;&#34;</span>  <span style="color:#75715e"># defensive; shouldn&#39;t happen</span></span></span></code></pre></div>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">

<div>

</div>

<div>

</div>

</div>



  



  
  
  


 
        
  
  <div class="book-comments">

</div>
  
 
        <div class="text-center p-0">
	<ul id="nav-social" class="list-inline">
		
			<li class="list-inline-item my-0 mr-3 ml-0 p-0">
				<a href="https://github.com/ppipada" target="_blank" aria-label="github">
					<i class="fab fa-github fa-1x text-muted"></i>
				</a>
			</li>
		
			<li class="list-inline-item my-0 mr-3 ml-0 p-0">
				<a href="https://www.linkedin.com/in/ppipada/" target="_blank" aria-label="linkedin">
					<i class="fab fa-linkedin-in fa-1x text-muted"></i>
				</a>
			</li>
		
			<li class="list-inline-item my-0 mr-3 ml-0 p-0">
				<a href="https://letterboxd.com/ppipada/" target="_blank" aria-label="letterboxd">
					<i class="fab fa-letterboxd fa-1x text-muted"></i>
				</a>
			</li>
		
			<li class="list-inline-item my-0 mr-3 ml-0 p-0">
				<a href="mailto:ppipada@gmail.com" target="_blank" aria-label="mail">
					<i class="fas fa-at fa-1x text-muted"></i>
				</a>
			</li>
		
			<li class="list-inline-item my-0 mr-3 ml-0 p-0">
				<a href="https://pankajpipada.com/index.xml" target="_blank" aria-label="rss">
					<i class="fas fa-rss fa-1x text-muted"></i>
				</a>
			</li>
		
	</ul>
</div>

        
  
  <div class="book-copyright flex justify-center">
    <small>Tech Interview Prep - Copyright © 2016-Present Pankaj Pipada. All Rights Reserved.</small>
  </div>
  
 
        
  
  
    <script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script>
  

      </footer>

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
  
 
  </main>

  
	
	<script src="https://pankajpipada.com/tech-interview-prep/js/copybutton.min.eb1935355b430030f89358fc7901ef972a3973b27cf70b5efba79813702b47ac.js" integrity="sha256-6xk1NVtDADD4k1j8eQHvlyo5c7J89wte&#43;6eYE3ArR6w=" defer></script>


</body>
</html>




















