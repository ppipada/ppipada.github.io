[
 {
  "content": "Adding search functionality to a static site generated with Hugo can significantly improve the user experience. lunr.js is a powerful JavaScript library for full-text search, offering a lightweight and fast solution ideal for static sites. In this guide, we will integrate search using lunr.js and address optimization concerns to ensure efficient and smooth operation. Note that the examples use Bootstrap and Font Awesome icons, but the same elements can be adapted to any styling system as required.\nStep 1: Create a data index for search In your config.toml, add the following lines to enable the generation of the search index file: [outputs] home = [\"HTML\", \"RSS\", \"JSON\"] [outputFormats] [outputFormats.JSON] mediaType = \"application/json\" baseName = \"index\" isPlainText = true Then, create a layouts/index.json file, that will have a template for creating the data index. This file will be processed during hugo build to create a {output dir e.g public}/index.json For example, if you want to have title, url, content, tags, date available, the template will look something like below: {{- $index := slice -}} {{- range $.Site.RegularPages -}} {{- $tags := slice }} {{- range .Params.tags -}} {{- $tags = $tags | append . }} {{- end -}} {{- $content := .Content | plainify | htmlUnescape }} {{- $datestr := .Date.Format \"Jan 2, 2006\" }} {{- $indexItem := dict \"url\" .Permalink \"title\" .Title \"content\" $content \"tags\" $tags \"date\" $datestr -}} {{- $index = $index | append $indexItem }} {{- end -}} {{- $index | jsonify (dict \"indent\" \" \") }} Step 2: Create the Search Form Now that we have the data created for search, we would need to establish a interaction mechanism with the user. You can embed the search form in the header or body of all pages or restrict it to a dedicated search page. It can be embedded as {{ partial \"search-form.html\" . }} This form should, take input from user, and on action, invoke the search page with search query embedded into the url. Below is a slightly opinionated layouts/partials/search-form.html, using Bootstrap classes and Font Awesome icons for styling. This can be adapted to any styling system. The form takes user input and invokes the search page with the search query embedded into the URL. \u003c!-- Form with a get page action --\u003e \u003cform id=\"search\" action='{{ with .GetPage \"/search\" }}{{.Permalink}}{{end}}' method=\"get\" class=\"d-flex justify-content-center mt-2 mb-4\"\u003e \u003c!-- Hidden label for accessibility --\u003e \u003clabel hidden for=\"search-input\"\u003eSearch site\u003c/label\u003e \u003cdiv class=\"input-group\" style=\"width: 90%;\"\u003e \u003c!-- Icon inside the input group for visual enhancement --\u003e \u003cspan class=\"input-group-text border-0 bg-transparent\"\u003e \u003ci class=\"fa fa-search\"\u003e\u003c/i\u003e \u003c/span\u003e \u003c!-- Search input field. The \"name\" defined here will be used to parse the URL when executing the business logic in search.js --\u003e \u003cinput type=\"text\" class=\"form-control rounded-pill\" id=\"search-input\" name=\"query\" placeholder=\"Type here to search...\" aria-label=\"Search\"\u003e \u003c!-- Submit button with an arrow icon --\u003e \u003cbutton class=\"btn border-0 bg-transparent\" type=\"submit\" aria-label=\"search\"\u003e \u003ci class=\"fa fa-arrow-right\"\u003e\u003c/i\u003e \u003c/button\u003e \u003c/div\u003e \u003c/form\u003e Step 3: Set Up Your Search Page To actually execute the query and display the results we need to create a search page. Generally it would be content/search/_index.md, but this can change depending on your chosen site organization for Hugo. This file should point to a search layout, that we will create below. You can customize it as required. Typically a bare minimum file will look like: --- title: \"Search\" layout: \"search\" description: \"Search page\" --- Step 4: Create the Search Layout Now, the above page needs to be served using a layout. The search layout defines the structure of the search page and includes necessary scripts for lunr.js and the custom search logic. By including these scripts in the layout page only, we can ensure that the search functionality is loaded on this page only and doesn’t really affect other pages. This can help optimize performance for the rest of the site. Create a search.html file in your layouts/_default directory: {{ define \"main\" }} \u003cdiv id=\"search-container\" class=\"container\"\u003e \u003c!-- This is where the search results will be displayed. Initialize as empty list. --\u003e \u003cul id=\"searchresults\"\u003e\u003c/ul\u003e \u003c/div\u003e \u003c!-- Include lunr.js library. This can be included from node_modules mounted as assets/vendor, refer to their cdn or directly use from static/js. --\u003e {{ $lunrJS := resources.Get \"vendor/lunr/lunr.min.js\" }} \u003cscript src=\"{{ $lunrJS.RelPermalink }}\" defer\u003e\u003c/script\u003e \u003c!-- Include the custom search script where the magic happens. This can be used from assets/js like below, or directly from static/js. --\u003e {{ with resources.Get \"js/search.js\" }} {{ $minifiedScript := . | minify | fingerprint }} \u003cscript src=\"{{ $minifiedScript.Permalink }}\" integrity=\"{{ $minifiedScript.Data.Integrity }}\" defer\u003e\u003c/script\u003e {{ else }} {{ errorf \"search.js not found in assets/js/\" }} {{ end }} {{ end }} Step 5: Create the Search business logic script Now we need to connect all the above site elements to lunr.js, perform search and render results. We will create a javascript script for this. This script handles the entire search process, including loading the search index, processing search queries, and displaying results. The below script can be placed as assets/js/search.js and included in your search layout as shown in a previous step. Alternately, you can put it directly inside static/js folder too and include it via the search layout above. Flow of the Code Initialization: The script initializes the lunr.js search index and ensures it only happens once for a page load. Caching: It checks for cached search data in localStorage. If valid cached data is available, it uses it; otherwise, it fetches new data and caches it. Building the Index: The script constructs the lunr.js search index from the fetched data. Search Query Handling: It reads the search query from the URL parameters and triggers a search if a query is present. Search Execution: It performs the search using the built index and processes the query to ensure it is valid. Displaying Results: It limits the displayed results to a maximum of 10 to avoid overwhelming users and improve performance. // Get the search input element var searchElem = document.getElementById(\"search-input\"); // Define a global object to store search-related data and ensure it's initialized only once window.pankajpipadaCom = window.pankajpipadaCom || {}; // Initialize search only once if (!window.pankajpipadaCom.initialized) { window.pankajpipadaCom.lunrIndex = null; window.pankajpipadaCom.posts = null; window.pankajpipadaCom.initialized = true; // Load search data and initialize lunr.js loadSearch(); } // Function to load search data and initialize lunr.js function loadSearch() { var now = new Date().getTime(); // Check for cached data in localStorage var storedData = localStorage.getItem(\"postData\"); // Use cached data if available and not expired if (storedData) { storedData = JSON.parse(storedData); if (now \u003c storedData.expiry) { console.log(\"Using cached data\"); buildIndex(storedData.data, checkURLAndSearch); return; } else { console.log(\"Cached data expired\"); localStorage.removeItem(\"postData\"); } } // Fetch search data via AJAX request var xhr = new XMLHttpRequest(); xhr.onreadystatechange = function () { if (xhr.readyState === 4 \u0026\u0026 xhr.status === 200) { try { var data = JSON.parse(xhr.responseText); buildIndex(data, checkURLAndSearch); console.log(\"Search initialized\"); // Cache fetched data with expiry localStorage.setItem( \"postData\", JSON.stringify({ data: data, expiry: new Date().getTime() + 7 * 24 * 60 * 60 * 1000, // TTL for 1 week }) ); } catch (error) { console.error(\"Error parsing JSON:\", error); showError(\"Failed to load search data.\"); } } else if (xhr.status !== 200) { console.error(\"Failed to load data:\", xhr.status, xhr.statusText); showError(\"Failed to load search data.\"); } }; xhr.onerror = function () { console.error(\"Network error occurred.\"); showError(\"Failed to load search data due to network error.\"); }; xhr.open(\"GET\", \"../index.json\"); xhr.send(); } // Function to build lunr.js index function buildIndex(data, callback) { window.pankajpipadaCom.posts = data; window.pankajpipadaCom.lunrIndex = lunr(function () { this.ref(\"url\"); this.field(\"content\", { boost: 10 }); this.field(\"title\", { boost: 15 }); this.field(\"tags\"); this.field(\"date\"); window.pankajpipadaCom.posts.forEach(function (doc) { this.add(doc); }, this); }); console.log(\"Index built at\", new Date().toISOString()); callback(); } // Function to display error message function showError(message) { var searchResults = document.getElementById(\"searchresults\"); searchResults.innerHTML = `\u003cbr\u003e\u003ch2 style=\"text-align:center\"\u003e${message}\u003c/h2\u003e`; searchElem.disabled = true; // Disable search input on error } // Function to check URL for search query and perform search function checkURLAndSearch() { var urlParams = new URLSearchParams(window.location.search); var query = urlParams.get(\"query\"); if (query) { searchElem.value = query; showSearchResults(); } } // Function to perform search and display results function showSearchResults() { if (!window.pankajpipadaCom.lunrIndex) { console.log(\"Index not available.\"); return; // Exit function if index not loaded } var query = searchElem.value || \"\"; var searchString = query.trim().replace(/[^\\w\\s]/gi, \"\"); if (!searchString) { displayResults([]); return; // Exit if the search string is empty or only whitespace } var matches = window.pankajpipadaCom.lunrIndex.search(searchString); console.log(\"matches\", matches); var matchPosts = matches.map((m) =\u003e window.pankajpipadaCom.posts.find((p) =\u003e p.url === m.ref) ); console.log(\"Match posts\", matchPosts); displayResults(matchPosts); } // Function to display search results function displayResults(results) { const searchResults = document.getElementById(\"searchresults\"); const maxResults = 10; // Limit to 10 results if (results.length) { let resultList = \"\"; results.slice(0, maxResults).forEach((result) =\u003e { if (result) { resultList += getResultStr(result); } }); searchResults.innerHTML = resultList; } else { searchResults.innerHTML = \"No results found.\"; } } // Function to format search result items function getResultStr(result) { var resultList = ` \u003cli style=\"margin-bottom: 1rem\"\u003e \u003ca href=\"${result.url}\"\u003e${result.title}\u003c/a\u003e\u003cbr /\u003e \u003cp\u003e${result.content.substring(0, 150)}...\u003c/p\u003e \u003cdiv style=\"display: flex; justify-content: space-between; align-items: center; font-size: 0.9em; color: #6c757d; height: 1.2em; line-height: 1em; padding: 0.25em;\"\u003e \u003cdiv\u003e${result.date}\u003c/div\u003e \u003cdiv\u003e\u003ci class=\"fa fa-tags\"\u003e\u003c/i\u003e ${result.tags .map( (tag) =\u003e `\u003ca href=\"/tags/${tag}\" style=\"color: #6c757d;\"\u003e${tag}\u003c/a\u003e` ) .join(\", \")} \u003c/div\u003e \u003c/div\u003e \u003c/li\u003e`; return resultList; } Optimization Concerns Index Data Caching By default, the index is not preserved across page loads, which can result in unnecessary data fetching and processing. To improve performance, we can use localStorage to cache the search data. This caching mechanism is already implemented in the loadSearch function, where data is stored with a time-to-live (TTL) of one week. This ensures that the index is only fetched and built once a week, reducing the load on the server and improving user experience. Note that localStorage needs data to be in a serializable format and hence the index directly cannot be cached. Therefore, we are caching the index.json post data that we created in the first step and then rebuilding the index at each window creation. Limiting Search Results Limit the number of search results displayed to the user to avoid overwhelming them and to improve performance. This is done by taking a maxResults length slice above. Async Loading of Scripts To improve page load times, ensure that the search scripts are loaded asynchronously.\nThis is achieved by adding the defer attribute to the script tags in the layout.\n\u003cscript src=\"{{ $lunrJS.RelPermalink }}\" defer\u003e\u003c/script\u003e \u003cscript src=\"{{ $minifiedScript.Permalink }}\" integrity=\"{{ $minifiedScript.Data.Integrity }}\" defer\u003e\u003c/script\u003e Note that this deferring means the page will first render and then the actual search execution will begin.\nQuery-Based Search Our layout doesn’t really communicate with the script as such. It just loads the script. The script itself, sees if the data and indexes are present, then checks the URL for search query, then executes the search, modifies the html to add the result list items. It also helps to allow users to share searches easily. This is already handled in the checkURLAndSearch function, which reads the query parameter from the URL and performs a search if it’s present. Conclusion To recap, we created a search index data, a user interaction form, a search page and a layout for it. All this and lunr.js is tied together using a custom javascript. As noted before, the stylings used are bootstrap and font awesome based here, but can be easily adapted to any styling system. An implemented example of this can be found in this sites search functionality. Example search query ",
  "date": "May 27, 2024",
  "tags": [
   "hugo"
  ],
  "title": "Hugo - Integrate search using lunr.js",
  "url": "https://pankajpipada.com/posts/2024-05-27-hugo-search/"
 },
 {
  "content": "In Hugo, copying files to the output during the build process is a common task, especially when dealing with external resources like CSS, JavaScript, or source maps from node_modules. In this post, we will walk through how to achieve this without any additional external dependencies.\nResources Resources in Hugo are any files that can be processed by Hugo Pipes, such as images, stylesheets, and javascript files. These resources can be transformed, minified, concatenated, and otherwise manipulated using Hugo’s built-in functions. They are typically placed in the assets directory or other mounted directories. A common method to copy resources involves using resource.Get followed by Permalink, RelPermalink, or Publish. Here’s an example of how to copy a CSS file from node_modules to your output: First, mount node_modules to the assets folder. This can be done in config.toml:\n[module] [[module.mounts]] source = \"node_modules\" target = \"assets/vendor\" Note that if you do this, you have to explicitly mount other folders too. Defaults are shown in the Hugo documentation here .\nNow you have to refer to the mount path (i.e., assets/vendor above) when using Hugo pipes to process this file. For example:\n{{ $bootstrapCSS := resources.Get \"vendor/bootstrap/dist/css/bootstrap.min.css\" }} \u003clink rel=\"stylesheet\" href=\"{{ $bootstrapCSS.RelPermalink }}\"\u003e Non-Resources Non-resources are files that Hugo does not process automatically, such as certain source maps or configuration files. These files might be crucial for development or debugging but are not part of the standard resource pipeline in Hugo. For non-resources, the above approach doesn’t work directly because Hugo doesn’t create resources for them. Instead, you can follow these steps: Read the file. Use resources.FromString. Publish the resource. To optimize this process, you can create a partial and use it in your layout. Here’s an example partial that can be placed as layouts/partials/copy-sourcemap-from-nodemodules.html: {{ $mapFileOrig := . }} {{ $mapFileNode := printf \"/node_modules/%s\" $mapFileOrig }} {{ $mapFileVendor := printf \"vendor/%s\" $mapFileOrig }} {{/* warnf \"Source map in: %s, node path: %s vendor path: %s\" $mapFileOrig $mapFileNode $mapFileVendor */}} {{ $mapContent := readFile $mapFileNode }} {{ if $mapContent }} {{ $map := resources.FromString $mapFileVendor $mapContent }} {{ $map.Publish }} {{ else }} {{ errorf \"Source map not found: %s\" $mapFileNode }} {{ end }} To use this partial, include it in your template (generally baseof.html, etc) like this: {{ partial \"copy-sourcemap-from-nodemodules.html\" \"bootstrap/dist/css/bootstrap.min.css.map\" }} Note that during development, you may need to deleted the output folder and then build your site again for resource copy to work. Conclusion By following these steps, you can effectively manage and include both resources and non-resources in your Hugo projects, ensuring that all necessary files are available in the final output.\n",
  "date": "May 27, 2024",
  "tags": [
   "hugo"
  ],
  "title": "Hugo - Copying Files to Output Using Pipes",
  "url": "https://pankajpipada.com/posts/2024-05-27-hugo-copy/"
 },
 {
  "content": "In this post, we will explore several excellent resources for understanding LLMs. My focus is primarily text and code inference, but most of the things should be generic enough for everyone. The post is structured as a step-by-step learning journey. This is intended to be a regularly updated list.\n101 - Basic introductions a) Andrej Karpathy’s - Intro to Large Language Models This video provides a general and high-level introduction to LLMs, covering topics such as inference, scaling, fine-tuning, security concerns, and prompt injection. b) Nvidia’s Generative AI explained Note: This course would require you to login to nvidia first and then details are visible. This video gives a very high level overview of GenAI, its usage, applications that companies are targeting, etc. 102 - Courses that cover basic usage a) Microsoft’s Generative AI for Beginners I personally have found this to be a excellent course that touches LLMs, prompting, RAG, Agents, multi modals etc. b) Google’s Cloud Skills Boost Havent gone through this personally, but content looks ok. 103 - Prompt engineering Note that prompt engineering techniques may seem generic, but in practice every family of models generally have specific things that work for them. If you change models, the prompts may need to be adjusted for better results. (E.g Claude 2.x worked great with XML tags for constraints or 1 shot examples, GPT4 worked better with JSON)\na) Microsoft’s Introduction to prompt engineering b) OpenAI’s Prompt engineering guide c) Anthropic Claude’s Prompt engineering guide 201 - LLMs a) Andrej Karpathy’s State of GPT Covers things like tokenization to pre-training, supervised fine-tuning, and Reinforcement Learning from Human Feedback (RLHF). Also, practical techniques and mental models for the effective use of these models, including prompting strategies, fine-tuning, etc are covered. b) Visual Introduction: LLMs by Bycroft Excellent visualizations and explanations of LLMs using nanoGPT, GPT2, GPT3 Understanding the visualizations is slightly involved, most probably a 301 rather than 201, but glancing through also can help to some extent. 301 - Deep Dive into Gen AI / Machine Learning a) Andrew Ng’s Machine Learning Introduction The definitive course to dive into ML. Updated for covering GenAI too. (94 hrs) b) Fast.ai’s Course c) Andrej Karpathy’s Zero to Hero This is a youtube series that will build a GPT from scratch. d) 3Blue1Browns Season 3 - Neural networks Ideal course will be to take from season 1 to 4 to understand things end to end. e) Google’s Cloud Skills Boost for Advanced Devs I am skeptical about this learning path, but some folks have found this useful. 401 - Miscellaneous a) Stanford AI Courses b) Nvidias deep learning institute The institute has some interesting learning paths that are self paced. Free as well as paid content present. c) Tools and more extensive resources list at awesome-generative-ai Journey recommendation and conclusion It is important to tailor the learning journey based on personal interests and goals. If you are interested in consumption of LLM’s, having a basic understanding of concepts in 10x and 20x should be enough.\nFor folks interested specifically in text or code generation, I would recommend understanding below order of key areas:\nText Completion/Inference Prompt Engineering Tools/Function Calling RAG (Retrieve, Augment, Generate) Agents Fine Tuning Thinking of a use-case of interest and starting with coding it as early as possible along with above material gives best results in this authors experience.\nIf possible, avoid libraries (For Python, LLamaIndex and Langchain are the most popular ones as of now), when learning and try to write your own code. Calling APIs in python or any other language is generally very straight forward (chat gpt with gpt-3.5 should be able to give that code very easily).\nLibraries add a layer of abstraction that creates more impedance, than help during learning phase. I myself was able to pickup typescript, react, etc very easily using GPT’s and its APIs. One early experience is documented in this post . The open source FlexiGPT vscode plugin is an outcome of my personal AI journey.\nHappy learning !!!\n",
  "date": "Apr 15, 2024",
  "tags": [
   "gpt"
  ],
  "title": "Great Resources for Learning Generative AI and Large Language Models (LLMs)",
  "url": "https://pankajpipada.com/posts/2024-04-15-genai-resources/"
 },
 {
  "content": "Evolution In this ever-changing world, with shifting demands and responsibilities, I have experimented through multiple task management strategies. E.g: Zettelkasten (much more than task management, but still), Eisenhower matrix, Getting things done (GTD) method, Agile/Kanban, ABCDE method, Franklin planner method, etc.\nFinally, I’ve settled on a system that works for me (in most cases). It’s most likely a mash-up of various methods recommended from books, blog posts, conversations with experienced folks, etc. It may not be perfect and might even change in the future with changing roles and responsibilities, but it has served me well since quite some time.\nGranularity I mostly deal with unit items of tasks which are neither too small, nor too large.\n“Steps” to complete one item are not included (Not Too Small).\nProjects or ideas that might span a long time (months to years) are excluded (Not Too Large). These items and associated brainstorming don’t come into the task list. Parts of it only come here once these things materialize and are broken down into smaller units. Deliberation on a project, visions, designs, breakdowns into smaller items, are outside of task list.\nArrangement (prioritization) P-0: A very short list of things I would do “immediately”. Items that someone has told you to do immediately, but which you practically won’t be able to accomplish, should not be included (i.e. give me yesterday things). Stress Indicator: A large P-0 list means very high level of stress and is a sign of a problem that you need to address. P-1: Slightly larger list that contains the items that I need to address “sooner than later”. Many tasks enter directly into P-1. During peaceful times, P-0 is mostly populated from P-1. Although direct entries into P-0 can still happen, I prefer to avoid this as much as possible P-2: A large list of various obligations and tasks. These may be things I think I have to do, but they have no associated urgency and questionable motivation. This is an often neglected list. It turns out to be a “write and forget” list, with a hope attached that it “may” be used someday. General preference is to move things from here to associated project/ideas pages and reevaluate at that level rather than in task list. P-X: A short list of things that excite me. Keeping it short is crucial. If this grows, that means you I am not really excited by all of them and most probably few items need to be in P2. Daily focus (the heart of my system) Every day come up with tasks in two categories (~5/10 minutes daily time spend):\nDo: These are the tasks I have planned for the day, primarily a subset from P-0. Meetings are excluded from this for now. They tend to be obligatory and preplanned most of the time. I don’t see a lot of value in adding them as tasks. Look forward to: Items that excite me. Generally items come from P-0 or P-X here. If there are days when this list is empty, it’s a sign of a problem that I need to address. Good days are when there are more than 1 item here. Tools Tried and discarded: Google keep: It served well enough in very early stages. Somehow the “features” added increased and its usefulness decreased. I still use it for lists or things I need to refer to once in a while, but not for tasks. Roam research/Notion: Tried them for a very short time and most probably very early in their lifecycle. Never got over the initial barrier. Editor plugins Was interested in this as my writing in Markdown increased over time. Multi device availability and less suitability for projects/ideas deliberation were a problem. Kanban boards Tried it out just for the heck of it at some point. Looking at a board at the start of the day was a very bad feeling generally. Current: Microsoft OneNote Got introduced to it via a colleague when I shifted to Mac from Linux. I can easily keep most things in one “Page”. Daily “Do/Look forward to” list can be created in minimal time. Moving around tasks is very easy. Project/ideas/Meeting notes pages can be kept in same place. Sync is available across my devices (Mac/Android). There are a lot of features there that I have almost never used. Hope is that the primary workflow and UX for note-taking remains more or less same. Parting note Interestingly, this model doesn’t apply to my family life at all. But, I find myself following it about 80% of the time in my work/career life. It’s a delicate balance between the immediate necessities and the sparks of excitement that keep me going.\nThe red flags, the problems, the overwhelming moments — all find their reflection in this system. It is not necessary that this model reduces stress per se. Depending on the way my career was going the P-0 list was very overwhelming at some points. And even though I was aware of the red flags, I couldn’t do much about it.\nI hope sharing this provides a window into my thoughts and methods, but it’s essential to remember that this is purely my experience. What works for me might not be applicable to anyone else, as it’s a product of my unique journey and professional life.\n",
  "date": "Jul 30, 2023",
  "tags": [
   "life"
  ],
  "title": "Finding my balance: An evolved and simplified task management system",
  "url": "https://pankajpipada.com/posts/2023-07-30-taskmgmt/"
 },
 {
  "content": "Blunt/Direct communicators A lot of engineers (especially individual contributors) get characterized as blunt or direct or assertive communicators. These individuals generally communicate their thoughts and opinions in an unfiltered and straightforward manner. The desired outcome from a conversation these folks expect is clarity. Sugar-coating, diplomatic phrasings, adding vagueness, etc. are generally seen as detrimental to the conversation. Getting straight to the point generally seems important to them, rather than dance around a topic.\nA lot of people on the receiving end of these communications start perceiving this as rudeness. Even though the intention to hurt or belittle others may not be present, a perception of sabotage or hurt ego is very common. The necessary directness added to the conversation generally also is looked up as a superiority complex or ego centric-ness.\nOnline communication and its challenges In the increasingly remote work environment world, video calls (zoom, teams or otherwise), instant messaging (slack, etc.) and emails e omnipresent and their adoption is increasing. Even though these modes of communication facilitate a lot of necessary buffers between individuals (very necessary for introverts, another quality that is very much present in the same sample space of individuals), it presents some unique challenges like:\nTone Misinterpretation: Emails and text-based communication platforms lack the intonation, pauses, and emphasis that vocal speech allows. This absence leads to misinterpretations, with straightforward comments often perceived as harsh or impolite. In my personal experience, this is the largest source of “escalations”. Perceived Lack of Empathy: Direct communication can be efficient, but it might also be viewed as lacking empathy or consideration for others feelings. Loss of Non-Verbal Cues: In face-to-face communication, a direct individual often relies on physical cues like body language or tone, to soften the impact of their words or gauge the listener’s reaction. These cues are generally absent in online communication. These leads to, often missing or misunderstood interpretation by the receiver about the intention behind the message. Delayed Feedback: The delayed responses in a lot of these communication modes, leads to misunderstandings or create uncertainty about how a message has been received. Overemphasis on Content Over Relationship: Blunt speakers often prioritize the message’s content over the relationship with the receiver. This can further aggravate feelings of disconnect or misalignment. Formality and Expectations: A blunt person’s tendency to be concise and to-the-point comes off as rudeness in an environment where a certain level of formality or ‘small talk’ or ‘diplomacy’ is expected. Suggested solutions The below solutions/strategies have been suggested to me multiple times and through multiple forums (performance reviews, 1:1 feedbacks, escalation RCAs, leadership trainings, GPT, etc)\nEmbrace Emotional Intelligence: Be aware of the emotional impact of one’s words and use this understanding to frame the communication in a more palatable way. My experience: This is hard, like really hard. It definitely works, but doing it itself is a large challenge. Leverage Emojis/GIFs: Emojis/GIFs can be used to inject humor, soften a statement, or show empathy, which can prevent misinterpretations. My experience: This helps a bit in instant messaging. It is detrimental in almost all other settings. Seek Feedback and Clarify: Checking in or seeking feedback with the receiver can clear up potential misunderstandings and help demonstrate the intention to communicate respectfully. My experience: Feasibility to do this itself is often very less. It mostly works with someone you have an already established rapport with. Utilize Video Calls: Video calls can be a substitute for fave-to-face communication. My experience: Not useful other than 1:1 conversations. Develop Your Written Communication Skills: The written word lacks the subtlety of spoken language. Thus, learning to use phrases that convey respect, empathy, and openness can help to soften the impact of a direct communication style. My experience: Definitely necessary to know how to communicate your point “clearly”. But asking to stop being direct in written communication is a circular solution i.e stop being direct. Things that worked Apart from the above experiences, the below things have worked for me in varied settings. (Still a long way to go…):\nDetailed write-ups Writing down proposals/discussion points in detail with proper context setting, options considered, conclusions, sending these docs in advance and then discussing these as needed helps a lot. However, the challenge is that few people want to read lengthy write-ups. Of course this is not applicable to all scenarios, but if done a bit tactically, gives great results. Audio calls with discussion being done on a written medium While discussing a topic, if there are written points, which are projected on the screen, it helps a lot in streamlining the discussion and getting the point across in a much more palatable way. If it is an open discussion, just projecting a blank doc/editor and a facilitator writing down points, actions, pros/cons etc. helps a lot in everyone staying on track and getting to a conclusion. Digital whiteboards/drawing tools have always given worse results with lots of frustrations for me. Avoid any direct personal remarks While not everyone who is direct does it, but it is very easy to call out a person for a mistake or lack of knowledge. This is never ever a good idea. Maybe some 1:1 setting, but mostly should be avoided. Avoid satire Again, may not be a general thing, but never works in an online communication setting. Parting thoughts Seeking efficiency, transparency and honesty can be valuable in many contexts, especially in situations that require decisiveness, clarity, and swift action. Though it has its own challenges, just remembering that there is a human on other end of it can help a long way. More often than not, both the parties want to achieve the same goal.\n",
  "date": "Jul 10, 2023",
  "tags": [
   "life"
  ],
  "title": "Blunt/Direct/Assertive communicators, online communication challenges and how to overcome them",
  "url": "https://pankajpipada.com/posts/2023-07-10-assertive/"
 },
 {
  "content": "Today, I delved into Paul Graham’s (PG) How to do great work essay. He provides great insights into the topic and extensively discusses it (and I mean extensively…). However, there seems to be an aspect that is not addressed in the conversation – the people who are in survival mode or have recently emerged from it and how this affects their ability to do great work.\nWhen someone is in survival mode, they are primarily focused on meeting basic needs such as food, shelter, and healthcare. This can make it challenging to pursue personal interests, hobbies, or ambitious career goals because so much energy and resources are dedicated to just getting by. It’s a significant issue that many people face. Those who have just come out of survival mode are confronted with the challenge of unlearning some of the habits that got them out of survival mode and try to adjust to the new state, often while still carrying the mentality of survival mode.\nLet’s take a look, one by one, at the article’s arguments and how the above two could affect how to do great work.\nThe four steps PG prescribes the following four steps:\nChoose a field that aligns with your natural aptitude and deep interest (leads to harder work and diligence). Learn about your chosen field until you reach the frontier of knowledge. (Hard work needed). Notice the gaps in knowledge that others might overlook. Explore these gaps (fractal buds), especially the ones that others aren’t interested in. (Hard work needed). Now if you are in survival mode, the first step needs to be modified to: “Choose a field that aligns with your natural aptitude, can help you get by, and you have a deep interest in”. If the deep interest part doesn’t directly correlate with the ‘sustain’ aspect, you will most likely prioritize sustainability; but try to cultivate your interest over time as you stabilize your life. It may not be that difficult if the aptitude part of it is preserved.\nFor those who have recently exited survival mode, they have likely made some compromises in the first step. The challenge lies in breaking free from the survival mentality and allowing themselves to pursue fields that may not immediately contribute to their survival but align with their interests and aptitudes.\nEmbracing curiosity and adaptability PG touches upon the intricate journey of discovering one’s true calling. He argues that understanding the essence of a profession requires immersion and experience, often leading to a long, overlapping process of exploration, learning, and self-realization. He prescribes engaging in a broad spectrum of experiences, fostering curiosity and openness to increase the chances of discovering one’s passion. PG argues that if a field fails to captivate you as you delve deeper, it’s likely not your true calling. The importance of adaptability and the courage to change paths when a more exciting opportunity presents itself is highlighted. Finally, he warns against the distractions of societal pressures and external influences.\nFor survival mode and the life thereafter folks, the most difficult part remains about avoiding distractions due to societal pressures. This gives rise to the additional constraint of needing to ‘get by,’ and a marked increase in the difficulty level of adaptability and courage. For the folks that are just out of survival, the fear of falling back to it is the biggest deterrent.\nOvercoming Inertia and Procrastination Subsequently, PG touches upon navigating the journey of work requiring strategic management of time and energy. He argues that overcoming the initial inertia to start work often requires self-deception, like underestimating project complexity. He states that completion of projects is vital as it often leads to the most valuable outcomes. Furthermore, he warns of per-project procrastination, which can disguise itself as productivity and suggests regular self-checks to help stay on track.\nThe strategy of self-deception can be particularly useful for those transitioning out of survival mode, who may face additional challenges in starting new projects due to the lingering survival mentality.\nExcellence, consistency and long term value PG highlights the importance of aiming for the best in your field. This can be a challenging but rewarding goal for those in survival mode and for those transitioning out of survival mode.\nConsistency in doing great work is crucial. It’s not about getting a lot done every day, but about getting something done consistently. This principle holds true even more for those in survival mode or transitioning out of it. Every small step taken consistently can lead to significant progress over time.\nPG encourages us to aim to create something that will still be valued in a hundred years. This long-term perspective can guide your work and help ensure its lasting impact. For those transitioning out of survival mode, this perspective can provide a beacon of hope and a powerful motivation to strive for greatness, helping them break free from the survival mentality.\nUnlearning misconceptions and embracing experience In the journey from survival mode to a state of thriving, PG emphasizes the need to shed misconceptions and embrace the wisdom of experience. He challenges the passive learning model ingrained by traditional education systems, advocating for an active approach where educators are seen as advisors rather than authority figures.\nFor those transitioning out of survival mode, this shift in perspective can be empowering. It encourages self-reliance and autonomy, essential traits when navigating life beyond survival. PG warns against seeking shortcuts or ‘hacking the test’ for success, a mindset often adopted in survival mode. Instead, he emphasizes that real achievement comes from addressing overlooked problems and producing quality work.\nPG also advises against depending on external validation or ‘gatekeepers’ for success. This is particularly relevant for those emerging from survival mode, who may be accustomed to seeking approval or assistance from others. Instead, he encourages focusing on self-improvement and producing quality work.\nFinally, PG highlights the importance of learning from both positive and negative examples and the value of transferring ideas from one field to another. This can be particularly beneficial for those transitioning out of survival mode, as it encourages flexibility and adaptability, key traits for thriving in new environments.\nThe influence of people In the pursuit of greatness, the surrounding people can significantly shape your journey. Colleagues who inspire and challenge you can stimulate your growth and push you towards your goals. As you transition from survival mode, it’s crucial to surround yourself with individuals who fuel your optimism and maintain high morale. This positive cycle can enhance your work and drive you towards success. In my small personal experience, individuals who you can look up to due to their own dedication, focus and optimism are a great source of positive energy. Moreover, individuals that are in similar financial position as you and still are driven by things other than money are great motivations.\nYour audience, even if small and dedicated, can provide the necessary motivation and feedback for continuous improvement. Their appreciation and support can be a powerful catalyst, especially when transitioning out of survival mode.\nWhile prestige can be appealing, it’s important to remember that the value of your work should not be solely determined by others’ opinions. Instead, focus on excelling in your chosen field and making it prestigious through your own efforts. Curiosity, a powerful guide, can lead you to new discoveries and achievements.\nIn this journey, the influence of the right people can be a game-changer.\nSummary As you transition from survival mode to a state of thriving to doing great work, remember that the discoveries are out there, waiting to be made. Embracing the right internal and external shifts can go a long way.\n",
  "date": "Jul 2, 2023",
  "tags": [
   "life"
  ],
  "title": "Survival mode, life immediately after and Paul Graham's How to do great work",
  "url": "https://pankajpipada.com/posts/2023-07-02-great-work/"
 },
 {
  "content": "Background I got into espresso a few years back. I started out simply enjoying an office machine espresso, but as my interest grew, I found myself venturing to local cafes (these were sparse in early days, but the number has grown over the years in my town) and eventually crafting my own brew at home, over a span of 7 to 8 years.\nAs of today, I own a Baratza Virtuoso Plus grinder And a Gaggia Classic Pro coffee machine. I generally keep on trying different types of coffee bean roasts and origins.\nAlthough I’ve not conclusively established a favorite, my preference leans toward medium roasts. My top choices for coffee beans include either Gianyar or Kintamani Bali and Antigua Guatemala. When it comes to darker roasts, Sumatra and Ethiopia are my go-to beans.\nBeing the lone espresso consumer in my household, I typically brew a couple of espresso shots in the morning and, occasionally, an additional pair in the afternoon. It’s essential to note that the brewing method may would differ based on the frequency of usage of the machine. The below recipe is for a single daily home brew.\nThe Espresso Recipe Start by measuring 14g of coffee beans for a double shot. Set the grind size on your Baratza Virtuoso Plus grinder to 6 for medium roasts. You may adjust it to 6 or 8 for dark roasts, depending on the specific bean. Proceed to grind the beans. Switch on your Gaggia Classic Pro machine. Attach an empty portafilter to the machine and let it warm up until the first “brew” light illuminates. Activate the brew button to dispense the first cup of hot water. Once the light is off, turn off the button. Allow the machine to heat up again for the second time, and repeat the process to brew out a second cup of water. Remove and dry the portafilter using a kitchen cloth. Add the freshly ground coffee to the portafilter, and use a coffee tamper to compact the grinds. For best results, use a heavy tamper that fits the portafilter size perfectly. Reattach the filled portafilter to the machine. Activate the steam button to preheat the machine for 15 seconds. Turn off the steam button. The brew light will immediately illuminate. Turn on the brew button to start brewing. Allow the brew to process for about 20 seconds. Ideally, your brew should be a dark brown color, topped with a rich crema. I hope you enjoy this espresso journey as much as I have. Whether you’re a seasoned coffee lover or a newcomer, the beauty of coffee making is in the experience and the flavorful sip you earn at the end. Happy brewing!\n",
  "date": "Jun 2, 2023",
  "tags": [
   "life"
  ],
  "title": "My Go-to Espresso Recipe - An Everyday Brew",
  "url": "https://pankajpipada.com/posts/2023-06-02-espresso/"
 },
 {
  "content": "Dear friend,\nI am not sure why I am writing this now. I have thought about it many times, but never really got around to doing it. You have been on my mind since a few days now for some reason, and I have finally decided to write something.\nIt’s been 2 years 7 months on this day. I am not sure if I have processed things cleanly until now. I would most probably never be able to forgive myself for not doing more to help you. I am very sorry for that. Most probably I have had a lot of excuses for it.\nYou were a great friend. You were always trying to help others find happiness and make them feel comfortable. Mostly because of it, you had so many friends to share your good times. We did not share the same interests/vices, but I always felt great comfort in knowing that we could talk to each other freely and mostly about any topic under the sun, personal or otherwise. But, as I learned about some things that you were dealing with, I am not so sure anymore. I wish I could have given you more confidence so that you would open up about those things too with me. I wish I would have taken that COVID pass and come to meet you. I wish you never left our company. I wish you never moved out from your brothers place.\nThank you for becoming a friend in a sea of colleagues. Thank you for teaching me your out of the world debugging skills. Thank you for helping me with communicating more openly with people.\nI pray that you have found your peace.\nYour friend.\n",
  "date": "May 25, 2023",
  "tags": [
   "life"
  ],
  "title": "Remembering my friend",
  "url": "https://pankajpipada.com/posts/2023-05-25-remembering/"
 },
 {
  "content": "Introduction Have you ever wanted to learn a new programming language or framework but felt overwhelmed and unsure of where to start? Pair programming can offer a solution to this challenge by allowing you to learn from a more experienced developer in real-time. In this case study, we will explore the pair programming experience between me and ChatGPT in the context of learning about “Admonitions” in Hugo.\nRelevant background I have some small amount of experience in web development, and I am familiar with using Hugo, a popular open-source static site generator. My primary day job experience is with backend technologies and architecture. I am an absolute beginner when it comes to CSS styles, placing and using them in hugo, writing HTML using styles and hugo specific functions.\nI have been wanting to add an admonition that adds each quote in a box, to my quote’s page in this blog for some time. With the very limited experience I have in this area, I was not able to create a clean solution for it until now. With rise in ChatGPT’s and some experimentation with it, I decided to take on this task.\nWhat are Admonitions in Hugo? Admonitions are blocks of text that emphasize particular information and enhance the visual appeal of a document. They are commonly used to create notes, warnings, tips, and other types of annotations. Admonitions in Hugo can be created using the “shortcodes” feature, which allows you to add custom content to a page using a simple syntax.\nInteraction with ChatGPT Step 1: Set context Me \u003e Explain admonitions in hugo, give samples related to it, and guide on how to create them. (This was through multiple questions, but no code from my side.)\nChatGPT \u003e ChatGPT provides step-by-step instructions on how to create Admonitions in Hugo using the built-in shortcodes feature and custom CSS styles. I can see an example of adding quotes with HTML and associated CSS, as well as modifying the CSS to highlight the author name differently. ChatGPT also provides HTML code for creating a blockquote shortcode and modifying it to take the author name as an argument rather than as a separate span.\nStep 2: Get code for my problem with input code hints Me \u003e Style the blockquote admonition to match my hugo theme. This also through multiple questions, but this time, relevant code was provided each time.\nChatGPT \u003e I could learn how to modify the CSS styles to match my specific Hugo-themed-Bootstrap theme. ChatGPT provides CSS code for styling the blockquote elements to match the styles used in the theme, including the background color, border color, padding, and font styles for the author name.\nMe \u003e I was unsure on exact placement of CSS files and using them in shortcode in the theme. I asked ChatGPT about it.\nChatGPT \u003e ChatGPT also explains where to add CSS styles to a shortcode in Hugo. I could also learn about the steps to create a CSS file and include it in the HTML template for the site. This time too it gives relevant code blocks.\nStep 3: Non-ChatGPT steps needed At this point, I had a working code, but the font awesome icons were missing in the output as the theme had the icon in fas set and ChatGPT’s response had been guiding me to use it from the fa set. Identifying and rectifying them through ChatGPT interaction turned out to be very difficult. I had to go through the general route of exploring themes code, Stack Overflow search through Google, and then fixing it.\nStep 4: Block edit using ChatGPT instead of regex Me \u003e At this point a fully functional and tested shortcode was created. I now needed to use it across all the different quotes I had. Without ChatGPT, I would need to do a regex replace for doing some edits and then do some manual edits where regex was not really possible due to the slightly ad hoc nature of placement of information. I asked ChatGPT to do what I wanted as plain text command and provided input of my full text file.\nChatGPT \u003e ChatGPT was able to do the task of replacing the old “highlight” built-in admonition, with blockquote admonition, replace the author name from previous text field to the admonition parameter field perfectly. It was not able to reformat a few quotes into Markdown again properly. I decided to manually do that as it was a very minor task.\nExperience on guiding ChatGPT You can see that I needed to guide ChatGPT to the correct answer by asking specific and clear questions related to the topic of Admonitions in Hugo.\nI started by asking the basic question, then basic code, then edits to the code, then rectifying the code to match the specific environment. Even though ChatGPT helped to a large extent, identifying issues and whether the code generated and guidance is relevant to the consumers’ environment is definitely the user’s responsibility. Supplementing with additional context can assist, but the responsibility and the need for user knowledge remains unchanged.\nConclusion This interaction highlights the benefits of pair programming and learning, as it allows for a back-and-forth exchange of information and knowledge. Pairing with ChatGPT can be an effective way to learn about greenfield or brownfield programming concepts for a user.\nThough ChatGPT provides clear and concise instructions, examples, and code snippets to help the user understand and implement the concepts, as a consumer you need to be aware of what you want and what environment you are working with to get better results. There are some areas, where traditional debug and search Google/Stack Overflow is much easier path to solve the problem. Knowing when to move to the traditional method rather than spending time in ChatGPT is a skill to develop.\nWhether you are a beginner or an experienced developer, pair programming with ChatGPT can be a valuable learning experience.\nEpilogue This post itself was reviewed and modified using ChatGPT for spelling, grammar, sentence formulation, and structure. This task went fully without any hassle.\n",
  "date": "Feb 11, 2023",
  "tags": [
   "gpt",
   "hugo"
  ],
  "title": "Beginner level learning and pair programming with ChatGPT - A case study of Admonitions in Hugo",
  "url": "https://pankajpipada.com/posts/2023-02-11-pairprogram/"
 },
 {
  "content": "Links Amazon DynamoDB at USENIX ATC 22 Amazon Dynamo at SOSP 2007 Notes Good insight into how the evolution worked from Dynamo to DynamoDB.\nHaving a fixed unwavering goal of providing a managed service with fast and predictable performance at any scale is great.\nSystem properties DynamoDB is a fully managed cloud service. DynamoDB employs a multi-tenant architecture DynamoDB achieves boundless scale for tables DynamoDB provides predictable performance. DynamoDB is highly available DynamoDB supports flexible use cases Lessons learnt from the evolution Adapting to customers’ traffic patterns to reshape the physical partitioning scheme of the database tables improves customer experience. Performing continuous verification of data-at-rest is a reliable way to protect against both hardware failures and software bugs in order to meet high durability goals. Maintaining high availability as a system evolves requires careful operational discipline and tooling. Mechanisms such as formal proofs of complex algorithms, game days (chaos and load tests), upgrade/downgrade tests, and deployment safety provides the freedom to safely adjust and experiment with the code without the fear of compromising correctness. Designing systems for predictability over absolute efficiency improves system stability. While components such as caches can improve performance, do not allow them to hide the work that would be performed in their absence, ensuring that the system is always provisioned to handle the unexpected. ",
  "date": "Oct 18, 2022",
  "tags": [
   "papers"
  ],
  "title": "DynamoDB paper",
  "url": "https://pankajpipada.com/posts/2022-10-18-dynamodb/"
 },
 {
  "content": "Recently a friend posed a question to our common group: What common problems do you face in building complex, evolving, maintainable systems? Below is the general path that this discussion flowed.\nBroad level architectural thought Main Architectural goal for building large, complex enough and evolving systems is almost always the same: Minimize the resources (people, machines) needed to accommodate change.\nTop level method for doing this is almost always: separation of concerns. Achieving separation of concerns needs you to make tradeoffs. These are people, process, product related. E.g: dev velocity, team coordination, system performance, scalability, availability, failure models, etc.\nAs your org/project grows, the optimal tradeoff point shifts and you do the corresponding changes to adjust to these shifting tradeoffs.\nMulti service integration thought The defined API contract needs to be designed so that it is stable. Proper Resource based rest APIs come in handy for this. This is generally a non-trivial, error prone task for a lot of people, as defining resources you are handling and operations on them for today and tomorrow is very difficult. Same goes for DB schema design in a single service context.\nOne school of thought says that don’t worry about tomorrows responsibility as it is impossible to predict. While a good advice, completely ignoring any forward compatibility thought leads to a lot of pain down the line is a general observation.\nSingle service/concern bounded thought Similar to Arch, major issue remain separation of concerns and tradeoffs you make.\nDev’s generally tend to start by mixing all things in a single function, class, package, etc. E.g; For a web service, people tend to do transport stuff (SSL, serialization, HTTP), business logic, database handling all as single methods in single place. For non web service process, people tend to handle any communication, threading, thread coordination, configuration, business logic, etc in single place. This mixing can be seen generally in different areas as below.\nObservability: Adding anything related to observability tends to disturb business logic. E.g If you want an api metric to be present, you should be able to do that without touching BL. It generally doesn’t happen that cleanly.\nState management and access: State handling is another common thing that starts as “accessible to all” as it is the simplest thing to start with. E.g: Make all states (Tables, files, blobs, etc) accessible to all functionality. As part of architectural evolution, you start by defining clear boundaries slowly slowly in terms of modules, packages, etc.\nClass/Package issues: People would generally find it very very difficult to define boundaries of packages, classes. This is common even if classes or packages are designed with private/public functional capabilities. E.g: If a function is exposed, should it really be exposed? Is that function part of the responsibility of the class/package?\nAs newer requirements pour into the system, the architectural, service interaction and service responsibility specific tradeoffs change.\nFew examples of these changes within a service boundary level are:\nChanging levels of abstraction - a new class is created out of one big one. This may result in routing calls. Preferred way to handle this is to create a new class, let callers integrate with it, in the mean time redirect from main class to here. If the cost of maintenance turns out to be high, you have to force clients to upgrade. One middle ground here is: provide a sdk, do the rerouting in sdk, ask clients to upgrade the sdk.\nInterface change: especially if parameters are removed. This may result in building a stub to manage it. This is preferably handled via versioning. Backward incompatible changes need to upgrade major versions. Old version stays until you deprecate it. In a single codebase, modifying the callers is almost always preferred over handling rerouting, stubs, etc. Versioning is used when you don’t control the callers. Again, tradeoff is cost of modifying everybody, vs maintaining reroutes.\nSize of teams vs rules/patterns One thing that I believe is that the rules/patterns to handle change don’t really change. What changes is the tradeoffs associated with picking a solution.\nReferences Blog at a abstract level: Patterns of legacy displacement Details about patterns are present in sidebar. Critical Aggregator Divert the Flow Extract Product Lines Feature Parity Legacy Mimic Revert to Source Transitional Architecture Books that I like: Software Architecture the hard parts Clean Architecture Architectural bookshelf with different levels/context of the problems: Architect bookshelf Architect library all ",
  "date": "Oct 18, 2022",
  "tags": [
   "systems-design"
  ],
  "title": "Separation of concerns and architectural thought",
  "url": "https://pankajpipada.com/posts/2022-10-18-archlevels/"
 },
 {
  "content": "Interests For a large mount of time I used to be quite voracious in different areas of interests/hobbies. These were different things at different points of time in my life. Movies, music, books about technology and science fiction, building software systems were the main interests until now.\nFor books, the voraciousness lasted a bit longer for technology ones and to some extent for science fiction. I didn’t really get to a large number of non-tech books, but the general approach to go about it was very much similar i.e pickup, continue to the end and don’t worry about much else.\nI am not sure when I developed a taste for movies. But I watched a lot of movies. There was a stretch of time, when I used to consume upto four/five per week minimum. TV series binge watch was also present to some extent, but movies were always the primary interest. At some point I decided to keep a track of what I watch on IMDB. As of now, I have almost 1550 titles to my watched/rated list (very few were added in the last few years though).\nI sought out music from anything and anywhere. Source of discovery came from recommendation engines, suggestions from select friends whose taste I trusted, even billboard top charts for each year. This exposed me to music of different moods and flavours. My playlists were generally a mixed jumble of eras and genres.\nBuilding software systems, that could handle things that I care about, has always interested me. It could be streaming services, local movie organization methods, writing systems that could juggle a lot of activities, software helping me organize my life in certain ways are a few areas. This also resulted in me growing interested in architecture and wanting to organize software itself for organizational goals.\nStages As an undergrad, my voraciousness mainly applied to movies, books about technology, music and some literature.\nDuring my masters, literature mostly dried up, and was replaced with building and managing different technology systems. I even managed to merge two of my interests to create a small movie streaming service for on-campus consumption by students.\nDuring my first job, books of all sorts were pretty much removed. With some dis-interest in the actual job, it was actually replaced with a feeling of void.\nMy second(current) job offered great learning and experimenting opportunities w.r.t building systems. That helped in filling the void to some extent. As I grew into the job, movies dried up to some extent. Music discovery stopped and it went into a mode of listening the same known things again and again. Books about technology did pick up as a job requirement, but the voraciousness to go about it was missing.\nChange As I progressed through my career, slowly but surely, the number of things I had to take care of increased. Upto a point, I was ok with the context switches and some associated parallelism. I think, things were ok until I was able to focus on a single thing for a while, and then switch the context to a new item. Even in this mode, the voraciousness was definitely lesser, but still present to some extent. Mainly because able to focus is one thing, and being able to do it in a voracious manner is another.\nSomehow, I also grew a bit rapidly on the engineering ladder. This directly corresponds to a increase in the amount of responsibilities and additional number of things to look into. Few people specialize in depth of a topic, but my interests in building systems meant I looked at breadth a lot, plus glue work associated with it. Initially I could go into large depths too with some limited breadth, but that changed as I grew. The scale of the issue could be grasped by considering that at times I handled more than a dozen active projects along with a few in maintenance. When this started overwhelming me, I tried out a model where I was deep into a couple of projects and rest all were in a consultancy mode. As the criticality of things increased I had to put few of them under the deep involvement mode rather than consultancy.\nThe company definitely rewarded the work in different ways. Also, I did lots and lots of things that were super interesting to me. Given that I actually enjoyed things, I was able to gain back some of the voraciousness as things permitted.\nCurrent thought I am starting to believe that even though I thrive when I handle a breadth of things, it is not sustainable for my mental health and general happiness. It is irrespective of the fact that, career wise, precisely this has helped me progress. I am not really sure of the solution to this at this point, even though I am aware of it and definitely struggling with it.\nI have been told that this type of change is definitely expected as you grow in technology. Whether, I accept it and find a way to live with it or make a change for gaining back that voraciousness is upto me. I am inclined towards the later as it seems to be the only way I can imagine gaining back satisfaction and happiness, but I am not sure about lateral implications of it.\nAnother suggestion was to see if there are different hobbies that can interest me and don’t really need me to be voracious about it. Out of the multiple things I tried gardening is the only one that has stuck and genuinely interested me. It doesn’t really feel like the only thing that is going to be there, but it will definitely be one to continue for a while.\nIF I find some list of newer things, along with a few old (I wouldn’t want to let music and movies to go away from me), keeping professional multi tasking to a minimum and finding a balance between professional and personal commitments and the interests could be the way ahead. In any case, it is really hard for me to imagine getting back on a thing that I can really be voracious about. Few friends have suggested that I don’t need to have voraciousness for happiness, but the thought of it makes me really uncomfortable.\nIt is definitely clear in my mind that I need to get rid of the dryness that exist in current procrastination, movie or music explorations. All this has affected my physical fitness too. Gaining that back to some extent has also been suggested as a thing that would help me freshen up a bit, but that too seems like a large arduous as of now.\nAll in all, the present looks like an uncomfortable time for me. Writing things down generally helps me build my own clarity and I hope to get the same out of this particular write up. Hopefully I achieve this clarity sooner rather than later.\nEdit 1: Suggestion by James from HN\nGood rest, taking care of your health (physical exercise, eating well, sleeping well), and lowering your stress levels. Find what you love right now and take small steps into recultivating doing something you love doing.\n",
  "date": "Jul 9, 2022",
  "tags": [
   "life"
  ],
  "title": "Voraciousness",
  "url": "https://pankajpipada.com/posts/2022-07-09-voraciousness/"
 },
 {
  "content": "I had to setup a fresh Ubuntu dev machine after quite some time. Given that this was a loaner machine, I wanted to make sure that I have a minimal viable dev setup ready as quickly as possible.\nBelow are the steps for the minimal things I need.\nUpgrade packages For a fresh setup it is better to first make sure that everything that is upgradable is up to date. If you have a specific version requirement for any package make sure you pin it at the package manager level.\nBasic commands\nsudo apt update sudo apt upgrade sudo apt install software-properties-common apt-transport-https wget zsh git vim tree sudo apt autoremove ZSH setup ZSH is an extended Bourne shell.\nTogether with Oh-My-ZSH it provides a delight full dev experience.\nI personally like to use the agnoster theme from ohmyzsh with the plugins git common-aliases zsh-syntax-highlighting zsh-autosuggestions\nFor Ubuntu terminal make sure you got to Terminal -\u003e Preferences -\u003e \u003cYour Profile\u003e -\u003e Colors and uncheck the Use system colors option so that the theme colors are used in the terminal.\nOhmyzsh standard plugins do not require explicit installation. Community plugins require some installation. Plugins links and installation guides are:\ncommon-aliases zsh-syntax-highlighting zsh-autosuggestions My simple ~/.zshrc file:\n# If you come from bash you might have to change your $PATH. # export PATH=$HOME/bin:/usr/local/bin:$PATH # Path to your oh-my-zsh installation. export ZSH=\"/home/username/.oh-my-zsh\" ZSH_THEME=\"agnoster\" # Which plugins would you like to load? # Standard plugins can be found in $ZSH/plugins/ # Custom plugins may be added to $ZSH_CUSTOM/plugins/ # Example format: plugins=(rails git textmate ruby lighthouse) # Add wisely, as too many plugins slow down shell startup. plugins=(git common-aliases zsh-syntax-highlighting zsh-autosuggestions) source $ZSH/oh-my-zsh.sh export HISTSIZE=100000 export SAVEHIST=100000 SSH key-gen and add to repositories Interacting with multiple hosted git repositories is much smoother when using SSH keys.\nSpecific git hosts provide their guides to do this, e.g: GitHub ssh key gen guide .\nGeneral setup includes:\nGenerate a ssh key pair on a machine. Add it to your git host profile settings. Test ssh access to git ssh-keygen -t ed25519 -C \"email@example.com\" ssh-keygen -t ed25519 -C \"username@example.com\" xclip -sel clip \u003c ~/.ssh/id_ed25519.pub # Add your git host (GitLab/GitHub/BitBucket, etc) URL for git-example.com ssh -T git@git-example.com Basic software install VSCode. General post for VSCode helpers is here Slack Zoom Hugo . Hugo is fantastic website building framework. Awesome for static sites. Microsoft ergonomic keyboard setup (4000 and 2019 ergonomic keyboard) All keyboard shortcuts can be configured using Settings \u003e Keyboard \u003e View and Customize shortcuts Volume keys are generally enabled by default. Recheck under Sound and media Favorite keys 1, 2 and 3 are already configured to Recent favorites of same number as Super + 1, .... To modify them to an application you can follow instructions in this StackOverflow question . tl;dr: Install dconf-editor, modify app-hotkey-x at org.gnome.shell.extensions.dash-to-dock. Reboot/Relogin after any changes via dconf-editor Media keys other than open media app (generally one that has right facing triangle inside a rectangle) can be configured using Sound and media For open media app: First disable the “Tools” key shortcut as mentioned in this StackOverflow question . tl;dr: Install dconf-editor, modify control-center-static at org.gnome.settings-daemon.plugins.media-keys. Reboot/Relogin after any changes via dconf-editor If you want to open the default media app, you can now do that using Sound and media \u003e Launch media player; If you want to set a custom application you can do that via Custom shortcuts \u003e + \u003e {name; e.g music}, {your music apps cli command ; e.g: youtube-music}, {Tools} For Calculator you can set it in Launchers. Snipping is equivalent to PrtScn. App switcher is equivalent to Activities or Tab switcher Search is equivalent to Activities i.e (windows button) Emoji needs to be set as a Custom shortcut \u003e + \u003e {Characters}, {gnome-characters}, {press the emoji button. It will come as Shift + Ctrl + Alt + Super + Space}. Office button can be set similarly to whatever you want. Programming language settings Go. Basic introductory primer is present here . Python. Getting started link Pull your code and go exploring :)\n",
  "date": "Jul 24, 2020",
  "tags": [
   "linux-utils"
  ],
  "title": "Linux - Ubuntu initial dev setup",
  "url": "https://pankajpipada.com/posts/2020-07-24-ubuntu-setup/"
 },
 {
  "content": "Below are the steps recommended to read the Paxos made simple paper by Leslie Lamport and understand Paxos.\nSteps Read full paper.\nPaxos made simple “Begin at the beginning,” the King said gravely, “and go on till you come to the end: then stop.” - The King. Alice in wonderland. Re-look at why is “leader election required”. 2.5 -\u003e 2.4 -\u003e 2.3\nRe-look the requirements for proposer P2c -\u003e P2b -\u003e P2a -\u003e P2\nRe-look the requirements for acceptor P1a -\u003e P1\nGo through the algorithm again. i.e Phase 1 and Phase 2.\nRepeat 2-4 again. Do 1-4, if still not sure.\n",
  "date": "May 25, 2020",
  "tags": [
   "papers"
  ],
  "title": "Understanding Paxos",
  "url": "https://pankajpipada.com/posts/2020-05-25-understanding-paxos/"
 },
 {
  "content": "Docsify, YAML front-matter, mustache templates \u0026 tags and some quirks when using them.\nDocsify Docsify is a great documentation site generator.\nIt generates your documentation website on the fly using Markdown files directly. To start using it, all you need to do is create an index.html and deploy it on GitHub pages or any other static site host. It has a great plugin system that enables extensibility and can be used for solving multiple use cases. Docsify-Mustache One such plugin that is greatly useful is Docsify-Mustache .\nIt allows preprocessing markdown documents with Mustache template engine. Mustache is a logic-less templating system. It works by expanding tags in a provided template using values provided in a hash or object. E.g: If you use {{name}} as template in your markdown and provide the value for name either via YAML front-matter or any other supported sources , that value will get rendered. How to use this plugin with docsify is very well explained in the documentation site for this plugin. Mustache tags Mustache supports different types of tags as documented in mustache manual . The basic tags, that were useful to me, when dealing with a documentation site are Variables, Sections with non-empty lists and Inverted sections. Below are a few examples on using these mustache tag types. I have considered the source of the “values” as YAML front-matter in markdown but as pointed out before, the source can be any supported input type. Variables Variables are the most basic tag type in mustache. The template for accessing a variable is {{variable_name}}.\nExample:\nConsider that you declare the following YAML front-matter in your markdown document:\n--- title: My awesome project documentation category: Useful --- Now, if you want to refer this in the markdown file you can add:\n{{title}} This project belongs to category: {{category}} When rendering the page title and category will be substituted using the values declared in the front matter. Sections with non-empty lists Sections render blocks of text one or more times, depending on the value of the key in the current context.\nA section begins with a pound and ends with a slash. That is, {{#person}} begins a \"person\" section while {{/person}} ends it.\nWhen the value for a section is a non-empty list, the text in the block will be displayed once for each item in the list. The context of the block will be set to the current item for each iteration. In this way we can loop over collections.\nIMPORTANT NOTE: If the list values contain a hyphen - in them then the list rendering is incorrect/fails for that value. To avoid this use single quotes (’’) around the value as depicted below.\nExample:\nConsider that you declare the following YAML front-matter in your markdown document:\n--- tags: [useful, \"rocket-science\", launch] --- Now, if you want to refer the tags list in the markdown file you can add:\nThis doc is has the following tags: {{#tags}} {{.}} {{/tags}} Output will be rendered as:\nThis doc is has the following tags: useful, rocket-science, launch Inverted sections Inverted sections may render text once based on the inverse value of the key. That is, they will be rendered if the key doesn’t exist, is false, or is an empty list.\nAn inverted section begins with a caret (hat) and ends with a slash. That is {{^person}} begins a “person” inverted section while {{/person}} ends it.\nExample:\nConsider that you declare the following YAML front-matter in your markdown document:\n--- tags: [] --- Now, if you want to handle the tags list being empty and check for category being absent you can add:\nThis doc is has the following tags: {{^category}} No category found !!! {{/category}} {{^tags}} No tags found !!! {{/tags}} Given that the categories key doesn’t exist and the tags list is empty, Output will be rendered as:\nNo categories found !!! No tags found !!! Extended example An extended example where we add a YAML front-matter to a markdown file, use the variables and handle the absent cases is given below.\nMarkdown file that adds a constant heading section to the documentation page, where the title will be displayed first, then category will be displayed and then the tags list is provided:\n--- title: My awesome project documentation tags: [useful, \"rocket-science\", launch] --- # {{title}} {{category}} {{^category}} No category found !!! {{/category}} Tag list: {{#tags}} {{.}} {{/tags}} {{^tags}} No tags found !!! {{/tags}} The output rendered will be:\nMy awesome project documentation No category found !!! Tag list: useful, rocket-science, launch Example project Most of the above mentioned concepts and tools are used at my tech-interview-prep project site. That can act as a good reference.\n",
  "date": "Apr 29, 2020",
  "tags": [
   "markdown"
  ],
  "title": "Mustache templates and YAML front-matter with Docsify",
  "url": "https://pankajpipada.com/posts/2020-04-29-docsify-mustache/"
 },
 {
  "content": "Basic shortcuts, useful plugins, and sample settings file for VSCode.\nThis is a short list that should help anyone to get started with VSCode. It does not go into advanced mode or doesn’t serve as a long “cheat sheet”. Intention is to help in getting started rather than doing advanced stuff.\nBasic shortcuts Description Mac Linux New file cmd(⌘) n ctrl n Search in file cmd(⌘) f Ctrl f Search across files cmd(⌘) shift(⇧) f ctrl shift f Go to Definition f12 f12 Go back ctrl(⌃) - ctrl - Go to file cmd(⌘) p ctrl p Open command pallette cmd(⌘) shift(⇧) p ctrl shift p Replace in file cmd(⌘) option(⌥) f ctrl alt f Replace across files cmd(⌘) shift(⇧) h ctrl shift h Plugins Name Description GitLens Git supercharged Python Full python support. Lint, debug, intellisense, format, etc Go Full Go support. Lint, debug, intellisense, format, etc Markdown All in One All you need to write Markdown. Keyboard shortcuts, table of contents, auto preview and more Markdownlint Markdown lint and style check Code spell checker Spelling checker for source code Prettier Prettier/Formatter for multiple formats Regex Description regex Search any character or new line `((. Group things and access in replace Add things in ( and ) and access as $1,… ",
  "date": "Apr 28, 2020",
  "tags": [
   "linux-utils"
  ],
  "title": "VSCode basic helpers",
  "url": "https://pankajpipada.com/posts/2020-04-28-vscode/"
 },
 {
  "content": "Few git commands.\nDescription Command Delete all branches locally except for ones having the word “master” `git branch Pull submodules initially git submodule update --init --recursive Update submodules git submodule update --recursive --remote Clone using username pass in URL `git clone http://${GIT_USERNAME}:$(echo -n $GIT_PASSWORD ",
  "date": "Mar 27, 2020",
  "tags": [
   "linux-utils"
  ],
  "title": "Linux - Git commands",
  "url": "https://pankajpipada.com/posts/2020-03-27-git-commands/"
 },
 {
  "content": "Generic references and points to consider when doing a memory analysis with go programming language.\nDo pprof memory profiling .\nGo returns memory to OS gradually. Typical time is ~5 minutes. If immediate return is needed, we could use FreeOSMemory . General recommendation is that if this is needed, manage memory alternatively.\nAlternative for frequent allocated and reused objects is sync.Pool usage. This is tricky and have to be careful when doing this. Sample usage can be found in blog .\nExcellant High performance go tips are avilable at this post . This talks about memory and GC too.\nGarbage collection behaviour is well explianed at: Ardan labs Post 1 , Post 2 and Post 3 . Also there is this keynote explaining the evolution of Go’s garbage collector.\n",
  "date": "Mar 24, 2020",
  "tags": [
   "golang"
  ],
  "title": "Memory analysis in Go",
  "url": "https://pankajpipada.com/posts/2020-03-24-go-memory-profiling/"
 },
 {
  "content": "Organizing a local storage based movie collection. At a high level it involves:\nPrepare movie metadata Renaming and folder arrangement Manage movies in Kodi Prepare movie metadata Use TinyMediaManager i.e tmm for metadata management\nStep 1: Prepare source folders:\nImport local movie paths These are called as sources in tmm. Add these via settings Clean duplicates Manage multi file movies Naming convention is important. Files should end with -cd1, -cd2, etc or -part1, -part2, etc. Bulk Subtitles find and download is generally a bad idea. Lot of inaccuracies in downloads. Better do this one by one and before any renaming of files. Step 2: Scrape metadata\nGenerally prefer Kodi format metadata (nfo) files. tmm provides option for this in the setting. IMDb vs TMDb metadata IMDb scrapping generally is slow for bulk operations TMDb provides a good source for movie metadata and most of it (including fan art and posters) can be downloaded from here. I prefer rating from IMDb. There is an option to select just ratings from IMDb in tmm I would recommend doing a two pass metadata search In the first pass do a bulk operation using TMDb scrapper. This can include TMDb ratings. Once all metadata is downloaded a second bulk search can be done just for ratings and top 250 data using IMDb. Renaming and folder arrangement This step move movies to a Kodi recommended structure using tmm. CAUTION Once renaming is complete it cannot be undone. Verify each and every thing before doing renaming. Recommended folder structure is movie-name (year). Generally prefer a flat structure over sub-directories and deep hierarchies. General preference for file names is same as folder name. I personally prefer movie-name (year) video-resolution imdb-rating part-no. Once folder name and file name preferences are set in Settings-\u003eMovies-\u003eRenamer, do a dry run to see the changes that tmm is going to do. Verify the dry run result carefully. Adjust settings if something seems weird. Execute rename once verified. Manage movies in Kodi Install Kodi on your laptop/desktop/tv. Import the media folder created using tmm above in Kodi media manager. As a general setting enable Update library on startup. Cleaning DB in Kodi is a bit of a pain. Avoid folder/movie path changes. Keep running clean library even for smaller changes done to files/folders. Periodically i.e once you have watched enough movies you may want to do a export library. This writes watched status and any extra info in Kodi to .nfo files in source. ",
  "date": "Mar 17, 2020",
  "tags": [
   "linux-utils",
   "movie"
  ],
  "title": "Movies - Organizing a largish movie collection",
  "url": "https://pankajpipada.com/posts/2020-03-17-organizing/"
 },
 {
  "content": " Lifes irreducible structure Paxos made simple The Google file system - GFS Amazon DynamoDB at USENIX ATC 22 Dynamo: Amazon’s Highly Available Key-value Store MapReduce: Simplified Data Processing on Large Clusters TAO: Facebook’s Distributed Data Store for the Social Graph Dapper, a Large-Scale Distributed Systems Tracing Infrastructure Bigtable: A Distributed Storage System for Structured Data Papers we love github repo ",
  "date": "Mar 12, 2020",
  "tags": [
   "papers"
  ],
  "title": "Awesome papers list",
  "url": "https://pankajpipada.com/posts/2020-03-12-awesome-papers/"
 },
 {
  "content": "Steps to set up a samba server on ubuntu 18.04.\nsudo apt update sudo apt install samba # Allow Samba in ufw firewall sudo ufw allow 'Samba' sudo systemctl status smbd # Create a directory to host Samba share sudo mkdir /disk1/samba #### User setup sudo useradd -M -d /disk1/samba/peewee -s /usr/sbin/nologin -G sambashare peewee sudo mkdir /disk1/samba/peewee sudo chown peewee:sambashare /disk1/samba/peewee sudo chmod 2770 /disk1/samba/peewee sudo smbpasswd -a peewee # set password here sudo smbpasswd -e peewee vi /etc/samba/smb.conf ## Add these to the globals section to avoid name mangling and using appropriate charset # [globals] mangled names = no dos charset = CP850 unix charset = UTF-8 [peewee] path = /disk1/samba/peewee browseable = yes read only = no force create mode = 0660 force directory mode = 2770 valid users = peewee sudo systemctl restart smbd sudo systemctl restart nmbd ",
  "date": "Jan 30, 2020",
  "tags": [
   "linux-utils"
  ],
  "title": "Samba setup",
  "url": "https://pankajpipada.com/posts/2020-01-30-samba-setup/"
 },
 {
  "content": "Few ubuntu server setup issues and corresponding steps to debug them.\nDisabling floppy drive: Error: print_req_error: I/O error, dev fd0, sector 0\nsudo rmmod floppy sudo echo \"blacklist floppy\" | sudo tee /etc/modprobe.d/blacklist-floppy.conf sudo dpkg-reconfigure initramfs-tools Error: Network not up: eth/ens doesnt show or no service network-manager\nsudo dhclient sudo apt update sudo apt install network-manager ifupdown sudo service network-manager restart sudo systemctl status NetworkManager.service sudo vi /etc/netplan/01-netcfg.yaml # This file describes the network interfaces available on your system # For more information, see netplan(5). network: version: 2 renderer: NetworkManager ethernets: ens32: dhcp4: yes sudo netplan generate sudo netplan apply ssh setup\n## For regenerating only rsa key sudo ssh-keygen -t rsa -b 4096 -f ssh_host_rsa_key ## For regenerating all missing keys sudo ssh-keygen # service ssh restart sudo systemctl status ssh Fix non-configured locales\nsudo locale-gen en_US en_US.UTF-8 sudo dpkg-reconfigure locales ",
  "date": "Jan 30, 2020",
  "tags": [
   "linux-utils"
  ],
  "title": "Linux - Ubuntu 18.04 server setup debug",
  "url": "https://pankajpipada.com/posts/2020-01-30-ubuntu-1804-server-debug/"
 },
 {
  "content": "Generic references to get started with go programming language.\nGetting started links Installation How to Write Go code Tour of Go Effective Go Reference documentation Project layout Package Oriented Design Style guide Standard project layout Naming conventions Standard Package names go blog Go Talk Few important rules: Package names are singular, short, clear, lower case, with NO under_scores or mixedCaps. Package content: Avoid stutter, simplify function names. Avoid meaningless package names such as util, lib, common, or misc. Function and variable names: The convention in Go is to use MixedCaps or mixedCaps rather than underscores to write multiword names. Keep local variables short. Common variable/type combinations can use really short names. E.g: ‘i’ - for index, ‘r’ for reader, ‘b’ for buffer. Acronyms should be all capitals: E.g: ServeHTTP and IDProcessor Validation: Use gofmt for autoformatting your code. Use golint . It would provide warnings related to naming and styling of go code. Code comments Standard Go Blog Few important rules: The convention is simple: to document a type, variable, constant, function, or even a package, write a regular comment directly preceding its declaration, with no intervening blank line. Godoc will then present that comment as text alongside the item it documents. The comment is a complete sentence that begins with the name of the element it describes. Comments on package declarations should provide general package documentation. Use doc.go for packages that need large amount of introductory documentation. Swagger documentation for APIs: Can use code annotations as described at GoSwagger Generic guidelines Writing clear idiomatic Go code: Effective Go Few important rules: Dependencies should be passed explicitly. E.g: Pass logger explicitly. Do NOT keep unused functions, constants, variables, types. Do NOT use panics/recover as exception catching mechanism. Avoid blanket error handling. Avoid Misusing errors. Avoid long functions. A function should not exhibit split personality. Avoid global objects. Validation: Static check Race detector Vet CI: GolangCI-Lint Testing Guidelines: Go provides command go test for running tests in \\*\\_test.go files. It also has support for benchmarking. go test has support for race detector using - race. Use it while running tests. Test file containing component tests should specify the build constraints so that files can be identifiable whether it has Unit or Component or Integration test. The build constraints will be helpful to exclusively run only UT or Component tests or Integration tests. Command “go test ./… -tags UT” will only run unit test file and will exclude file component_test.go as it as define build constraint “!UT” E.g File component_test.go // +build !UT … some tests Tools: Package for boiler plate test code generation: gotests This is available as a plugin in VSCode and other major IDEs. Package for generating mocks for interfaces: mock Package for mocking SQL: go-sqlmock Test helpers: testing Measuring code coverage Go test can be configured -cover to collect code coverage information. It does not have support to find code coverage in workflow testing. But can be tweaked to collect coverage information by writing test for entry point function of binary. ",
  "date": "Nov 17, 2019",
  "tags": [
   "golang"
  ],
  "title": "Go introductory primer",
  "url": "https://pankajpipada.com/posts/2019-11-17-go-intro-primer/"
 },
 {
  "content": "Basic docker commands.\nCommands Description Command List all container instances, with their ID and status docker ps -a Lists all images on the local machine docker images Displays the logs from a running container docker logs [container name or ID] Stop all containers docker stop $(docker ps -a -q) Delete all containers docker rm $(docker ps -a -q) Changes command prompt from the host to a running container docker attach [container name or ID] Executes a command within a running container docker exec [container name or ID] shell command ",
  "date": "Nov 11, 2019",
  "tags": [
   "linux-utils"
  ],
  "title": "Docker - Commands",
  "url": "https://pankajpipada.com/posts/2019-11-11-docker-commands/"
 },
 {
  "content": "Random collection of commands for Ubuntu Linux.\nTop Description Command display top with threads top -H top with output sorted by memory top -o %MEM run top in batch mode 10 times with 5 seconds delay in command mode with output sorted by memory top -b -n 10 -d 5 -c -o %MEM run top in batch mode 10 times with 5 seconds delay in command mode with output sorted by memory and only print 15 lines at a time `top -b -n 10 -d 5 -c -o %MEM Jekyll Reference - Using with bundler Description Command serve jekyll locally bundle exec jekyll serve Virtualbox Description Command create a vmdk using raw device VBoxManage internalcommands createrawvmdk -filename \"\u003c/path/to/file\u003e.vmdk\" -rawdisk /dev/sdb Rsync Description Command rsync to a remote server using ssh protocol and show progress rsync -avzhe ssh --progress ./localfolder user@\u003cremote server name/ip\u003e:/remote/folder/location Option to transfer files upto a certain size --max-size=\u003cn\u003em Disk paritions Create a ntfs partition from an empty disk\nsudo fdisk /dev/sdb # fdisk is interactive # press m for help # Press p to list any available partitions # create a new partition by using n # after altering partitions press w to write # make a ntfs file system with \"quick format\" i.e dont write zeroes and dont check for bad sectors # remove f for full format mkfs.ntfs -f /dev/sdb1 blkid # Note the UUID of the partition E.g: /dev/sdb1 UUID=\"asdfg1246\" # Adding a entry in /etc/fstab UUID=asdfg1246 /disk1 ntfs-3g permissions,locale=en_US.utf8 0 2 ",
  "date": "Nov 10, 2019",
  "tags": [
   "linux-utils"
  ],
  "title": "Linux - Ubuntu - Random commands",
  "url": "https://pankajpipada.com/posts/2019-11-10-commands/"
 },
 {
  "content": "References, basic commands and sample rc file for GNU screen.\nReferences Man Page Quick Reference Basic commands Description Command Start a new session with session name screen -S \u003csession_name\u003e List running sessions / screens screen -ls Attach to a running session with name screen -R \u003csession_name\u003e Detach a running session screen -d \u003csession_name\u003e Command mode Ctrl+a Enable vertical scrolling mode in a running session Ctrl-a ESC Create new window Ctrl-a c Change to window by number Ctrl-a \u003cnumber\u003e Enter screen command Ctrl-a : Send command to the screen session screen -X -S \u003csession_name\u003e \u003ccommand\u003e Send kill command to the screen session screen -X -S \u003csession_name\u003e kill RC File # location: ~/.screenrc # A sample screenrc file with a hardstatus line at bottom # 3 windows created, with custom commands stuffed into each at session creation # and some key bindings. # the following lines give a status line, with the current window highlighted hardstatus alwayslastline hardstatus string '%{= kg}[%{G}%H%? %1`%?%{g}][%= %{= kB}%?%-Lw%?%{+b r}(%{G}%n*%f %t%?(%u)%?%{r})%{-b B}%?%+Lw%?%?%= %{g}%][%{B}%d/%m %{W}%C%A%{g}]' #hardstatus string '%{= kg}[ %{G}%H %{g}][%= %{= kB}%?%-Lw%?%{+b r}(%{G}%n*%f %t%?(%u)%?%{r})%{-b B}%?%+Lw%?%?%= %{g}%]' #hardstatus string '%{= kG}[%{G}%H%? %1`%?%{g}][%= %{= kw}%-w%{+b yk} %n*%t%?(%u)%? %{-}%+w %=%{g}][%{B}%d/%m %{W}%C%A%{g}]' # huge scrollback buffer defscrollback 10000 # no welcome message startup_message off # 256 colors # attrcolor b \".I\" # termcapinfo xterm 'Co#256:AB=\\E[48;5;%dm:AF=\\E[38;5;%dm' defbce on # mouse tracking allows to switch region focus by clicking # mousetrack on # default windows screen -t HOME 0 bash stuff \"cd /root\" screen -t SRV 1 bash stuff \"cd /root/srv\" screen -t MYSQL 2 bash stuff \"cd /root/mysql\" select 0 #bind c screen 1 # window numbering starts at 1 not 0 #bind 0 select 10 # get rid of silly xoff stuff #bind s split # navigating regions with Ctrl-arrows bindkey \"^[[1;5D\" focus left bindkey \"^[[1;5C\" focus right bindkey \"^[[1;5A\" focus up bindkey \"^[[1;5B\" focus down # switch windows with F3 (prev) and F4 (next) bindkey \"^[OR\" prev bindkey \"^[OS\" next # switch layouts with Ctrl+F3 (prev layout) and Ctrl+F4 (next) bindkey \"^[O1;5R\" layout prev bindkey \"^[O1;5S\" layout next # F2 puts Screen into resize mode. Resize regions using hjkl keys. bindkey \"^[OQ\" eval \"command -c rsz\" # enter resize mode 1,1 Top # switch layouts with Ctrl+F3 (prev layout) and Ctrl+F4 (next) bindkey \"^[O1;5R\" layout prev bindkey \"^[O1;5S\" layout next # F2 puts Screen into resize mode. Resize regions using hjkl keys. bindkey \"^[OQ\" eval \"command -c rsz\" # enter resize mode # use hjkl keys to resize regions bind -c rsz h eval \"resize -h -5\" \"command -c rsz\" bind -c rsz j eval \"resize -v -5\" \"command -c rsz\" bind -c rsz k eval \"resize -v +5\" \"command -c rsz\" bind -c rsz l eval \"resize -h +5\" \"command -c rsz\" # quickly switch between regions using tab and arrows bind -c rsz \\t eval \"focus\" \"command -c rsz\" # Tab bind -c rsz -k kl eval \"focus left\" \"command -c rsz\" # Left bind -c rsz -k kr eval \"focus right\" \"command -c rsz\" # Right bind -c rsz -k ku eval \"focus up\" \"command -c rsz\" # Up bind -c rsz -k kd eval \"focus down\" \"command -c rsz\" # Down #source .screen_layout #layout save def ",
  "date": "Nov 9, 2019",
  "tags": [
   "linux-utils"
  ],
  "title": "GNU Screen helpers",
  "url": "https://pankajpipada.com/posts/2019-11-09-screen/"
 },
 {
  "content": "A script to chain multiple closures in python with example usage.\nClosure chaining from functools import wraps # A new closure will be returned as a chain of Closures in the incoming list order def chain_closures(closure_list=None): if not closure_list: return None def chain(f): @wraps(f) def r(*args, **kwargs): newfunc = f for c in reversed(closure_list): newfunc = c(newfunc) return newfunc(*args, **kwargs) return r return chain Example from functools import wraps from closures import chain_closures import logging logger = logging.getLogger() def x(a): def request_logger(f): @wraps(f) def rlog(*args, **kwargs): logger.info(\"%s x entry\", a) ret = f(*args, **kwargs) logger.info(\"%s x exit\", a) return ret return rlog return request_logger def y(b): def request_logger(f): @wraps(f) def rlog(*args, **kwargs): logger.info(\"%s y entry\", b) ret = f(*args, **kwargs) logger.info(\"%s y exit\", b) return ret return rlog return request_logger def z(a, b): c1 = x(a) c2 = y(b) def request_logger(f): @wraps(f) def rlog(*args, **kwargs): q = c1(c2(f)) logger.info(\"z entry\") ret = q(*args, **kwargs) logger.info(\"z exit\") return ret return rlog return request_logger def w(a, b, f): c1 = x(a) c2 = y(b) q = c1(c2(f)) return q def get_w(a, b): c1 = x(a) c2 = y(b) q = chain_closures([c1, c2]) return q def f1(a, b): logger.info(\"%s %s\", a, b) # raise Exception(\"me except\") def smain(): c4 = get_w(10, 11) f2 = c4(f1) f2(12, 13) logger.info(f2.__name__) if __name__ == \"__main__\": logging.basicConfig(format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S', level=logging.DEBUG) smain() ",
  "date": "Nov 9, 2019",
  "tags": [
   "python"
  ],
  "title": "Python Helpers - Closures",
  "url": "https://pankajpipada.com/posts/2019-11-09-py-helpers-closures/"
 },
 {
  "content": "A script to cprofile a single function in python.\ncprofile a single function in python import logging import cProfile import pstats import StringIO from closure_chaining_example import smain logging.basicConfig(format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S', level=logging.DEBUG) pr = cProfile.Profile() pr.enable() smain() # do something pr.disable() s = StringIO.StringIO() sortby = 'cumulative' ps = pstats.Stats(pr, stream=s).sort_stats(sortby) ps.print_stats() logging.info(\"Profilestats: %s\", s.getvalue()) ",
  "date": "Nov 9, 2019",
  "tags": [
   "python"
  ],
  "title": "Python Helpers - Profiling",
  "url": "https://pankajpipada.com/posts/2019-11-09-py-helpers-profiling/"
 }
]

